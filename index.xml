<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>multiverse</title>
<link>https://amenalahassa.github.io/amenalahassa/</link>
<atom:link href="https://amenalahassa.github.io/amenalahassa/index.xml" rel="self" type="application/rss+xml"/>
<description>A small space in the vast universe of the Internet where I share the ideas and knowledge I&#39;ve gained on my journey into AI and software development.</description>
<generator>quarto-1.5.37</generator>
<lastBuildDate>Fri, 04 Oct 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>What I Learned from Calf Face 😊</title>
  <link>https://amenalahassa.github.io/amenalahassa/posts/lessons_from_calf_face.html</link>
  <description><![CDATA[ 




<section id="small-data-big-dreams" class="level2">
<h2 class="anchored" data-anchor-id="small-data-big-dreams">Small Data, Big Dreams</h2>
<p>Alright, let’s be real—when we started, we had a <em>tiny</em> data problem. We’re talking less than 200 images of calf faces (yep, calf faces!). And trust me, that’s nowhere near enough if we want to build a robust detection model. 🐮</p>
<p>But no worries, we had a plan! First, we took YOLOv8 and fine-tuned it with some clever data augmentation. We went from a measly 178 images to a whopping 890—thanks to a combo of <strong>GaussianBlur</strong>, <strong>MedianBlur</strong>, <strong>Sharpen</strong>, <strong>Flip</strong>, and good ol’ <strong>rotation</strong> (between 10 to 20 degrees, nothing too wild).</p>
<p>Now, the goal was to keep the transformations realistic. No crazy color changes or outlandish rotations that might produce data that doesn’t even exist in the real world—because that’s how you end up with a model that thinks cows fly. 🐄✈️</p>
<p>And guess what? After this magic data expansion, we saw a small bump in mAP50 (mean Average Precision) and other YOLO metrics. We went from 0.803 with basic data augmentation of YoloV8 to 0.891 on the mAP score after 10 epochs. 📈</p>
<p>Here’s a quick table showing the before and after, because data geeks love tables:</p>
<table class="table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mAP50</td>
<td>0.803</td>
<td>0.891</td>
</tr>
<tr class="even">
<td>mAP50-90</td>
<td>0.503</td>
<td>0.502</td>
</tr>
<tr class="odd">
<td>Precision</td>
<td>0.992</td>
<td>1</td>
</tr>
<tr class="even">
<td>Recall</td>
<td>0.778</td>
<td>0.769</td>
</tr>
</tbody>
</table>
<p>Now, before anyone screams “Overfitting!”—hold up. Yes, the boost in performance might make it seem like that’s what’s happening, but we’re confident that’s not the case. Why? Because the video data we plan to use for detection closely resembles the training data. In other words, the model’s doing exactly what we need it to do: detect calves in environments that are super similar to the ones it’s been trained on. 🎯</p>
<p>Oh, and by the way—the metrics you see in that table? Those aren’t based on the augmented data. Nope! They’re from a subset of the original 175 images, which we held back specifically as our test set. So, the numbers here reflect real, unaltered images, giving us a more honest assessment of the model’s performance. 💪</p>
</section>
<section id="choosing-the-right-model-training-where-things-get-real" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="choosing-the-right-model-training-where-things-get-real">Choosing the Right Model &amp; Training: Where Things Get Real 🎯</h2>
<p>Okay, so once we had the data sorted, it was time to pick the right model—and honestly, that’s where things got a little <em>complicated</em>. Not only did I need something that would crush the performance side of things, but I also needed to be able to explain <strong>why</strong> the model made the decisions it did. You know, in case anyone asks the big question: “Why did your model do that?” 😅</p>
<section id="the-model-line-up" class="level3">
<h3 class="anchored" data-anchor-id="the-model-line-up">The Model Line-Up 🏆</h3>
<p>For <strong>images</strong>, I went through a few options before landing on the right one. I tried <strong>DeepLabV3</strong>, <strong>EfficientNet</strong>, <strong>InceptionV3</strong>, and even <strong>Unet</strong> (which I ended up abandoning—long story 🫠). I also gave <strong>ViT</strong> and a <strong>LSTM+CNN combo</strong> a shot, but, well… let’s just say I didn’t get around to finishing that one. Too much on my plate!</p>
<p>For <strong>videos</strong>, it was another story. I experimented with <strong>TimeSformer</strong>, <strong>ViViT</strong>, and <strong>VideoMAE</strong> to handle the moving pictures. 🎥</p>
</section>
<section id="cracking-the-models-decisions-interpretability-with-omnixai" class="level3">
<h3 class="anchored" data-anchor-id="cracking-the-models-decisions-interpretability-with-omnixai">Cracking the Model’s Decisions: Interpretability with OmnixAI 🧠</h3>
<p>When it came to understanding why the models made specific predictions, I leaned on some awesome interpretability algorithms from OmnixAI. These tools helped me peek under the hood of the models and get a better sense of their thought process.</p>
<p>I used a mix of techniques like GradCAM, LIME, Score-CAM, and even SmoothGrad, GuidedBP, and LayerCAM to visualize what parts of the image the model was focusing on. Each one gave me a slightly different view of how the model was processing the data, which made interpreting results a whole lot easier (and way more fun to explain 😎).</p>
<p>And I also even trained <a href="https://github.com/Imageomics/INTR"><strong>INTR</strong></a>, a transformer-based model specifically designed for interpretability. ✨</p>
</section>
<section id="building-a-killer-dataset-split" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="building-a-killer-dataset-split">Building a Killer Dataset Split 🧩</h3>
<p>Next up, I had to make sure my dataset was split in a way that would allow for a solid comparison between models. Here’s how I did it:</p>
<p>Using the YOLO model I mentioned earlier, I went through almost 1 hour of video showing calves approaching a feeder. I grabbed the 10 seconds before each calf started feeding and extracted images where their cute little faces were visible. In the end, I had <strong>1,349 videos</strong> and <strong>7,687 images</strong>, representing <strong>76 unique calves</strong>.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/assets/images/paquetlab_video_dist.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Number of videos per calf status</figcaption>
</figure>
</div>
<p>To keep things clean, I handpicked <strong>68 videos</strong> where I verified that each calf was actually approaching the feeder (with no noise), and used those videos as my <strong>test set</strong>. The images from those videos? They became my <strong>image test set</strong> (283 images in total).</p>
</section>
<section id="two-different-training-sets-why-not" class="level3">
<h3 class="anchored" data-anchor-id="two-different-training-sets-why-not">Two Different Training Sets? Why Not? 🤷‍♂️</h3>
<p>I built two separate training sets just for fun (and science, of course):</p>
<ol type="1">
<li><strong>Full Set</strong>: This one included all the remaining images and videos, no holds barred.</li>
<li><strong>Limited Set</strong>: Here’s where things got interesting. I only used one video and two images per calf, per health status—“<strong>healthy</strong>,” “<strong>diarrhea</strong>,” or “<strong>pneumonia</strong>.” My logic? Since some images were super similar (I sampled about 30 images evenly from each 10-second video), I figured the model could easily overfit. I wanted to see how it would perform with less redundant data.</li>
</ol>
<p>Yeah, it’s a pretty <em>naive</em> approach, but it sped up my work and saved some headaches. Plus, even though about 40% of the data was, well, garbage 🗑️, the rest of it more than made up for it. 💪</p>
<p>I should mentioned that those calf in the test set were not in the training set, so the model never saw them before. 🐄</p>
</section>
<section id="pro-tip-use-mlflow" class="level3">
<h3 class="anchored" data-anchor-id="pro-tip-use-mlflow">Pro Tip: Use MLflow! 🛠️</h3>
<p>Oh, one last thing. If you’re planning on doing so much training like this, seriously, do yourself a favor and use <strong>MLflow</strong> to track your experiments. I didn’t use it for this project (don’t ask, it was a mess, I swear! 🤦‍♂️), but if I had to do it all over again, MLflow would’ve saved me SO much time and effort. Lesson learned!</p>
</section>
</section>
<section id="results-the-good-the-bad-and-the-calves" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="results-the-good-the-bad-and-the-calves">Results: The Good, the Bad, and the Calves 🐄</h2>
<p>You can check out all the detailed results and model performances in the dedicated repo for this <a href="https://github.com/amenalahassa/paquetlab">project</a> (because sharing is caring, right? 📂). But while I didn’t get a chance to completely wrap up the work (classic researcher life 😅), there are a few important highlights that are worth mentioning.</p>
<section id="the-curious-case-of-the-intr-model" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-curious-case-of-the-intr-model">The Curious Case of the INTR Model 🤔</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/assets/images/intr_teaser.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">How the INTR model works. From the original paper</figcaption>
</figure>
</div>
<p>So, let’s talk about the <strong>INTR</strong> model. The original paper boasted that this model could better explain its decisions, which sounded perfect for what I needed. But after fine-tuning it on my dataset? Yeah, not quite the same result. 🤷‍♂️</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/assets/images/intr_viz.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">How the INTR performs on my dataset</figcaption>
</figure>
</div>
<p>Why? Well, here’s where the <strong>data shift</strong> kicked in. The pre-trained model had been trained on images where the object of interest was neatly placed in the center of the frame (picture-perfect). My dataset? Not so much. My calves were sometimes, all over the place, doing their own thing, and not staying center-stage. 😬</p>
</section>
<section id="the-same-old-background-problem" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-same-old-background-problem">The “Same Old Background” Problem 🌾</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/assets/images/mean_face.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">Fun image of mean calf face. I used all images extracted from the videos to compute it.</figcaption>
</figure>
</div>
<p>Another issue I ran into: <strong>background consistency</strong>. Since all my training data came from the same farm, the background was pretty much always the same. So, while the model learned to perform decently well on that data, it didn’t generalize well when I tested it with images from a different farm. Different farms = different environments. And as we all know, no two farms are exactly alike, right? 🏡 🐄</p>
<p>It’s not the model’s fault—it’s just how things are. If the goal is to have a model that works across various farms, we either need a ton more data from different environments, or maybe we should consider building farm-specific models. Unless, of course, we want to standardize every farm, which, let’s face it, isn’t going to happen. 🤷‍♀️</p>
</section>
<section id="is-the-model-actually-learning-the-right-things" class="level3">
<h3 class="anchored" data-anchor-id="is-the-model-actually-learning-the-right-things">Is the Model Actually Learning the Right Things? 🤨</h3>
<p>Here’s the tricky part: The model performed relatively well. But—big but—it could be learning patterns we don’t want it to focus on. Maybe it’s using the <strong>calf’s face color</strong>, or maybe the <strong>background</strong> is playing a bigger role in its decision-making than we’d like to admit. In short, even though the performance metrics look good on paper, I’d recommend handling them with care. 📊</p>
</section>
<section id="words-of-wisdom-from-my-professor" class="level3">
<h3 class="anchored" data-anchor-id="words-of-wisdom-from-my-professor">Words of Wisdom from My Professor 👨‍🏫</h3>
<p>My supervisor always says we shouldn’t expect miracles from these models. After all, it’s hard for even a human to just look at a calf and determine its health status purely based on appearance. Add in the fact that we’re working with limited data, and yeah—it was always going to be a challenge.</p>
<p>But the whole point of this project was to test, explore, and see how a deep learning approach would hold up, despite those challenges. So, while the results may not be earth-shattering, the insights we’ve gained are super valuable for refining future models. 💡</p>
</section>
</section>
<section id="have-we-answered-the-big-question-can-we-detect-calf-health-status-from-images-or-videos" class="level2">
<h2 class="anchored" data-anchor-id="have-we-answered-the-big-question-can-we-detect-calf-health-status-from-images-or-videos">Have We Answered the Big Question? Can We Detect Calf Health Status from Images or Videos? 🤔</h2>
<p>Short answer? <strong>Not yet</strong>. 🛑</p>
<p>To be honest, I think we need a better dataset to get the results we’re aiming for. There’s definitely potential—maybe we can identify new features or patterns to help detect diseases more accurately, or perhaps the model can be trained to pick up on the same cues humans use when assessing calf health. But realistically, that’s going to take a lot more work than what I’ve done so far. 🐄💻</p>
<p>One major thing I’ve learned is that using pre-trained models (whether they’re foundation models or not) on real-life problems takes way more effort, data, and attention to detail than I ever expected. When you watch those flashy demos where models seem to perform flawlessly, it’s easy to think, “I got this.” But in reality? The process is a bit messier, and it involves a lot more tweaking than the demo might let on. 😅</p>
<p>Maybe I didn’t make all the right decisions—choosing the best model, or even formulating the right hypotheses—but hey, that’s how we learn, right? If you have thoughts, feedback, or think there’s something I missed, feel free to comment and let me know. Let’s keep the conversation going and figure this out together! 🤝</p>
</section>
<section id="and-thats-a-wrap" class="level2">
<h2 class="anchored" data-anchor-id="and-thats-a-wrap">And That’s a Wrap! 🎬</h2>
<p>Until next time, keep learning, keep growing, and keep exploring the world of AI. It’s a wild ride, but hey, someone’s gotta do it! 🚀</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a> ]]></description>
  <category>Work Projects</category>
  <category>Tips &amp; Tricks</category>
  <guid>https://amenalahassa.github.io/amenalahassa/posts/lessons_from_calf_face.html</guid>
  <pubDate>Fri, 04 Oct 2024 00:00:00 GMT</pubDate>
  <media:content url="https://amenalahassa.github.io/amenalahassa/posts/assets/images/intr_viz.png" medium="image" type="image/png" height="76" width="144"/>
</item>
<item>
  <title>How Much Memory Does Your Model Need on GPU? Let’s Find Out!</title>
  <link>https://amenalahassa.github.io/amenalahassa/posts/model_gpu.html</link>
  <description><![CDATA[ 




<section id="a-tool-to-estimate-model-memory-usage-on-gpu" class="level1 page-columns page-full">
<h1>A Tool to Estimate Model Memory Usage on GPU</h1>
<p>Hey there, fellow developers and curious minds! 🖖 Ever wondered how much juice you need to run a model on your GPU? Whether you’re knee-deep in code or just dipping your toes into the tech waters, the question of resources has likely crossed your mind.</p>
<section id="building-a-tool-to-estimate-gpu-memory-usage" class="level3">
<h3 class="anchored" data-anchor-id="building-a-tool-to-estimate-gpu-memory-usage">Building a Tool to Estimate GPU Memory Usage</h3>
<p>Guess what? I built a tool for that! 🎉 But hold up, before you get too excited… it only estimates the memory required to load the model onto a GPU. Yep, just that—no more, no less. If you’re itching to test it out, <a href="https://huggingface.co/spaces/konradhugging/model-gpu-estimator">here’s the link</a>.</p>
</section>
<section id="why-only-estimate-gpu-memory-for-loading" class="level3">
<h3 class="anchored" data-anchor-id="why-only-estimate-gpu-memory-for-loading">Why Only Estimate GPU Memory for Loading?</h3>
<p>You might be asking, “Why just the memory for loading?” Good question! It’s because to really figure out how much resources a model needs to run (whether training or inference), you actually need to run it. Yep, no shortcuts here.</p>
<p>It all depends on things like the size of your input/batch, the quality of your data (low-res images vs.&nbsp;high-res by eg), and a whole bunch of other factors. 🤯 Oh, and don’t forget the extra memory needed to store activations during the forward pass and gradients during the backward pass. The bigger the model, the more resources you’ll need. It’s a classic case of go big or go home!</p>
</section>
<section id="so-how-do-i-know-how-much-memory-my-model-needs" class="level3">
<h3 class="anchored" data-anchor-id="so-how-do-i-know-how-much-memory-my-model-needs">So, How Do I Know How Much Memory My Model Needs?</h3>
<p>Here’s the deal: The memory a model uses for training is not the same as it uses for inference. Training requires more memory because it needs to store activations and gradients for backpropagation. Unfortunately, there’s no one-size-fits-all rule to tell you exactly how much memory you’ll need, but we can make an educated guess.</p>
<p>You can start by using my tool to get a minimum memory estimate for loading the model. To estimate training memory, a rough approach is to multiply that number by 10, but keep in mind this is just a ballpark figure. The exact factor can vary depending on your model architecture and batch size. To be safe, always test with different configurations to find the sweet spot.</p>
<p>During inference, models generally require less memory and run faster compared to training, but there are exceptions. Monitoring tools like <code>nvidia-smi</code> can help you keep track of actual GPU usage and avoid surprises. 📊</p>
</section>
<section id="example-time" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="example-time">Example Time! 🎮</h3>
<p>Let’s take a practical example. I want to fine-tune the Qwen 2 model (Qwen/Qwen2-0.5B-Instruct) on a GPU P100 with 16GB of memory on Kaggle. For this exemple, I was trying to fine-tune the model on a dataset of 10k samples with a batch size of 2 and a maximum of 800 tokens per input, on a NER task.</p>
<p>When I load the model onto the GPU, it takes up around 2.6GB of memory. Not too bad, right? But when I try to train the model, you see in the image below, it maxes out all the resources. 🚀</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/assets/images/4.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">GPU Memory Usage</figcaption>
</figure>
</div>
<p>So, what’s the takeaway here? Even if your model loads fine, it might not run smoothly during training. Always keep an eye on your resources and adjust your model or batch size accordingly.</p>
</section>
<section id="v100-vs.-a100-vs.-whatever-gpu-what-difference-does-it-make" class="level3">
<h3 class="anchored" data-anchor-id="v100-vs.-a100-vs.-whatever-gpu-what-difference-does-it-make">V100 vs.&nbsp;A100 vs.&nbsp;Whatever GPU: What Difference Does It Make?</h3>
<p>So, you might be wondering, “Does it really matter which GPU I use?” Well, the short answer is: Absolutely! 🏎️</p>
<p>It’s like traveling from Canada to France. You could either fly or bike. Pretty obvious which one gets you there faster, right? The same logic applies to GPUs—more powerful ones will get the job done quicker, especially when running large models.</p>
<p>Let’s break it down:</p>
<ul>
<li><p><strong>Powerful GPUs like the A100</strong> not only handle large models better but also have extra features, like mixed-precision training, that can reduce memory usage and speed things up. They’re like the jet engines of the GPU world—fast, efficient, and capable of handling heavy workloads with ease.</p></li>
<li><p><strong>Older or less powerful GPUs like the V100</strong> can still do the job, but they might struggle with bigger models or more complex tasks. It’s like taking a slower flight—still gets you there, but not as fast or smoothly.</p></li>
<li><p><strong>Whatever GPU you have</strong>, just remember: You need both memory and raw power to make your model run efficiently. If your GPU isn’t powerful enough, your model might take longer to run or could even crash. And nobody wants that!</p></li>
</ul>
<p>So, whether you’re eyeing the latest A100 or making do with what you’ve got, the takeaway is clear: A more powerful GPU means faster, more efficient model runs. If you need to go deeper, you can read this <a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/">article</a> too.</p>
</section>
<section id="alternatives-if-your-model-is-too-big" class="level3">
<h3 class="anchored" data-anchor-id="alternatives-if-your-model-is-too-big">Alternatives If Your Model Is Too Big</h3>
<p>Got a huge model with over a billion parameters? No worries, there are alternatives! You can try building a smaller model that uses fewer resources. Techniques like transfer learning, model quantization, or even using a smaller model altogether can help.</p>
<p>However, keep in mind that these approaches might impact accuracy. Sometimes the trade-off isn’t that significant, and you can even set up safeguards for uncertain predictions, allowing the user to choose the most appropriate response. 💡</p>
<p>Fun fact: I’m working on a tool to anonymize text before feeding it to a large language model (LLM), and I use one of these techniques to run the model directly in the browser. Check it out here!</p>
</section>
<section id="takeaways" class="level3">
<h3 class="anchored" data-anchor-id="takeaways">Takeaways</h3>
<p>Alright, let’s wrap things up:</p>
<ol type="1">
<li>Estimating exact memory requirements for your model can be tricky, but my tool can give you a good starting point.</li>
<li>The more powerful your GPU, the faster and smoother your model will run—so choose wisely!</li>
<li>If your model is too big, consider alternatives like downsizing or using techniques that reduce resource demands, though be mindful of potential accuracy trade-offs.</li>
</ol>
</section>
<section id="goodbye-and-thanks" class="level3">
<h3 class="anchored" data-anchor-id="goodbye-and-thanks">Goodbye and Thanks!</h3>
<p>Thanks for sticking with me through this post! I hope you found it helpful. If you have any thoughts, suggestions, or just want to say hi, feel free to reach out via email or drop an issue below. I’m always open to learning and would love to hear from you.</p>
<p>Take care, and happy coding! 🚀</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a> ]]></description>
  <category>Tips &amp; Tricks</category>
  <category>Personal Projects</category>
  <guid>https://amenalahassa.github.io/amenalahassa/posts/model_gpu.html</guid>
  <pubDate>Sat, 24 Aug 2024 00:00:00 GMT</pubDate>
  <media:content url="https://amenalahassa.github.io/amenalahassa/posts/assets/images/0_1.png" medium="image" type="image/png" height="189" width="144"/>
</item>
<item>
  <title>How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)</title>
  <link>https://amenalahassa.github.io/amenalahassa/posts/anonymizer_intro.html</link>
  <description><![CDATA[ 




<p>Hey folks! 🙌</p>
<p>Ever find yourself using those cool new LLMs like ChatGPT for pretty much everything—whether it’s writing emails, drafting cover letters, or even creating blog content (I know I do! 😅)? It’s like having your personal assistant right there in your browser, making life so much easier. But, wait…what happens when your prompts contain personal details? 😬</p>
<p>You know, that awkward moment when you realize you’ve shared too much in a prompt? Yeah, been there, done that. So, I figured, why not build a tool that handles this issue and keeps things private? 💡</p>
<section id="how-i-found-myself-working-on-this" class="level2">
<h2 class="anchored" data-anchor-id="how-i-found-myself-working-on-this">How I Found Myself Working on This 🛠️</h2>
<p>Recently, I noticed just how much I’ve been relying on LLMs for personal stuff—writing emails, tweaking my CV, crafting blogs, even coding! But every time I use them, I have to manually remove or anonymize personal info in my prompts. Talk about tedious! 😩</p>
<p>It got to a point where I was doing this so often that I thought, “There’s got to be a better way!” And so, the idea of building a tool that runs entirely offline, right in your browser, was born. 🚀</p>
</section>
<section id="the-idea" class="level2">
<h2 class="anchored" data-anchor-id="the-idea">The Idea 💡</h2>
<p>It’s super simple! Just paste your text, and voila—it anonymizes it. Now, you can safely use it in your prompts without worrying about personal details leaking out. ✨</p>
<section id="example-time" class="level3">
<h3 class="anchored" data-anchor-id="example-time">Example Time! ✍️</h3>
<p>Imagine you’ve have to send an email to a customer about an upcoming event at your coffee shop. Here’s what it might look like:</p>
<blockquote class="blockquote">
<p>Subject: Join us for the Grand Renovation of New York - Exciting Changes Await!</p>
<p>Dear John Green,</p>
<p>We hope this email finds you well and enjoying your favorite vanilla latte large and banana muffin at our coffee shop! As a valued and loyal customer, we are thrilled to share some exciting news with you.</p>
<p>Mark your calendar for January 4, 2023, as we invite you to join us for the grand renovation of our coffee shop at New York! We are embarking on a journey to create an even better and cozier space for you to relax, indulge, and enjoy your favorite beverages and treats.</p>
<p>You have been a cherished part of our coffee shop family for an incredible 5 years, and we cannot wait to show our appreciation by unveiling a fresh, modern look that will elevate your coffee shop experience. The renovation will bring about stylish and comfortable seating areas, captivating artwork, and state-of-the-art equipment to enhance the quality of our offerings.</p>
<p>The grand reopening event promises to be a celebration filled with delightful surprises. As our special guest, you will be among the first to explore the redesigned space and sample our latest menu additions. Immerse yourself in the inviting ambiance, meet our talented baristas, and indulge in the aroma of freshly brewed coffee that is synonymous with our coffee shop. We value your opinion, John Green, and would be thrilled to hear your thoughts and feedback on our newly renovated space. As always, our commitment remains to provide you with exceptional customer service and the highest quality products that you know and love.</p>
<p>Save the date: January 4, 2023! We cannot wait to welcome you to the grand renovation of our coffee shop at New York. Together, let’s embark on this exciting journey and create countless new memories over aromatic coffees and delectable treats.</p>
<p>Thank you for being an integral part of our coffee shop’s success. We look forward to continuing to serve you and make your coffee experiences unforgettable.</p>
<p>Warmest regards,</p>
</blockquote>
<p>You paste it into the tool, and <em>poof</em>—it replaces those details with anonymous IDs. 🕵️‍♀️</p>
<blockquote class="blockquote">
<p>Subject: Join us for the Grand Renovation of aLOCATION_524a - Exciting Changes Await!</p>
<p>Dear aPEOPLE_872a,</p>
<p>We hope this email finds you well and enjoying your favorite vanilla latte large and banana muffin at our coffee shop! As a valued and loyal customer, we are thrilled to share some exciting news with you.</p>
<p>Mark your calendar for aDATE_988a, as we invite you to join us for the grand renovation of our coffee shop at aLOCATION_524a! We are embarking on a journey to create an even better and cozier space for you to relax, indulge, and enjoy your favorite beverages and treats.</p>
<p>You have been a cherished part of our coffee shop family for an incredible 5 years, and we cannot wait to show our appreciation by unveiling a fresh, modern look that will elevate your coffee shop experience. The renovation will bring about stylish and comfortable seating areas, captivating artwork, and state-of-the-art equipment to enhance the quality of our offerings.</p>
<p>The grand reopening event promises to be a celebration filled with delightful surprises. As our special guest, you will be among the first to explore the redesigned space and sample our latest menu additions. Immerse yourself in the inviting ambiance, meet our talented baristas, and indulge in the aroma of freshly brewed coffee that is synonymous with our coffee shop. We value your opinion, aPEOPLE_872a, and would be thrilled to hear your thoughts and feedback on our newly renovated space. As always, our commitment remains to provide you with exceptional customer service and the highest quality products that you know and love.</p>
<p>Save the date: aDATE_988a! We cannot wait to welcome you to the grand renovation of our coffee shop at aLOCATION_524a. Together, let’s embark on this exciting journey and create countless new memories over aromatic coffees and delectable treats.</p>
<p>Thank you for being an integral part of our coffee shop’s success. We look forward to continuing to serve you and make your coffee experiences unforgettable.</p>
<p>Warmest regards,</p>
</blockquote>
<p>Then, you can use that anonymized text in your LLM prompt. Once you get the LLM’s response, you paste it back into the tool, and it replaces the IDs with your original info. As simple as that! 😎</p>
</section>
</section>
<section id="how-to-try-it" class="level2">
<h2 class="anchored" data-anchor-id="how-to-try-it">How to Try It? 🤔</h2>
<p>Excited to give it a spin? You can check it out <a href="https://amenalahassa.github.io/web-anonymiser/">here</a>. It’s still a work in progress (so don’t judge me too harshly! 😜), but the full offline version is coming soon.</p>
<p>For now, you can test it out and share your thoughts with me. Your feedback means the world! 🌍</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps 🚀</h2>
<p>Here’s what I’ve got cooking for the future:</p>
<ul>
<li>Turning it into a browser extension that automatically catches text you paste into input boxes, helping protect your identity on the fly.</li>
<li>Extending the tool to handle PII (Personally Identifiable Information) with customizable options for what to anonymize.</li>
<li>And so much more! 🎉</li>
</ul>
</section>
<section id="but-why" class="level2">
<h2 class="anchored" data-anchor-id="but-why">But Why? 🤷‍♂️</h2>
<p>Good question! There are already some awesome tools out there, like <a href="https://www.pontus.so/blog">this one</a> and <a href="https://github.com/AI-LLM/AnonymizedGPT?tab=readme-ov-file">this GitHub repo</a>. But none of them are quite what I was looking for—so I decided to share mine! 😄</p>
<p>If you’ve got any comments, feel free to reach out or open an issue. I’m all ears! 👂</p>
</section>
<section id="it-was-fun-and-educational" class="level2">
<h2 class="anchored" data-anchor-id="it-was-fun-and-educational">It Was Fun and Educational! 🎓</h2>
<p>Building this tool was such a learning experience. I dove into ONNX, WebGPU, WebAssembly, Transformer.js, Hugging Face Inference, Angular 18, and so much more. And, boy, did I learn a lot! 💻</p>
<section id="also-a-few-things-i-noticed" class="level3">
<h3 class="anchored" data-anchor-id="also-a-few-things-i-noticed">Also, a Few Things I Noticed… 🧐</h3>
<ul>
<li>Running an LLM in the browser without a GPU can be <strong>super slow</strong>. 🐢</li>
<li>Not all browsers support WebGPU, so that’s a challenge.</li>
<li>To run a model in the browser, you need a <strong>really light model</strong>, preferably quantized and optimized for the web (like with Olive, for example). Ideally, it should run on WebGPU for the best performance.</li>
</ul>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final Thoughts… 🎉</h2>
<p>Overall, it’s been a fantastic experience. Version 2 is already in the works, so stay tuned! 😎</p>
<p>Thanks so much for reading, and don’t hesitate to reach out with your thoughts.</p>
<p>Cheers! 🥂</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a> ]]></description>
  <category>Tips &amp; Tricks</category>
  <category>Personal Projects</category>
  <guid>https://amenalahassa.github.io/amenalahassa/posts/anonymizer_intro.html</guid>
  <pubDate>Fri, 23 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Finally, An Introduction: The Story Behind This Blog and What’s Next</title>
  <link>https://amenalahassa.github.io/amenalahassa/posts/for_you.html</link>
  <description><![CDATA[ 




<section id="what-happened-indeed" class="level2">
<h2 class="anchored" data-anchor-id="what-happened-indeed">What Happened Indeed? 🤔</h2>
<p>Alright, let’s be real. Life happened, right? 😅 I took some time to wrap up a few things in my life and, honestly, to kickstart a new chapter. You know that feeling when you finish one thing and feel like a whole new world opens up? Yeah, that’s where I’m at! 🌟</p>
<p>Plus, it was the end of the school session, and I needed some well-deserved downtime. Relaxing, de-stressing, and just enjoying life for a bit. But guess what? Now I’m back, energized, and ready to take on the world! 🌍✨</p>
<p>I’ve realized how much I’ve already accomplished, and let me tell you, I’m super grateful for all of it. 🙏 But I also see how much more there is to do. My blog, my job, my personal life – they all need my attention, and I’m ready to dive in.</p>
<p>The possibilities ahead? Endless. I’m seizing every opportunity coming my way, and I can’t wait to share the journey with you! 🚀</p>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s Next? 🎉</h2>
<p>First off, a huge shoutout to my brothers and my family. Without them, I wouldn’t be here doing what I love. You guys are the real MVPs! 💪</p>
<p>Now, onto me. I’m committed to working on myself, becoming the best version of me. I’m just getting started on my journey, my career, my dreams – and I’m 100% ready to make them a reality. Let’s do this! 🙌</p>
</section>
<section id="the-reason-behind-this-blog" class="level2">
<h2 class="anchored" data-anchor-id="the-reason-behind-this-blog">The Reason Behind This Blog 💻</h2>
<p>So, why this blog? Well, it’s simple. I want to share what I’ve accomplished in my career, in AI, in software engineering – and maybe some other areas too. 🚀</p>
<p>But here’s the thing: this isn’t going to be a blog where I show you how to do things or preach about the future of AI (though, who knows, maybe a little bit of that 😉). Instead, it’s going to be a place where I share what I’m learning, what I’m building, and who I’m building it with. And hey, I’d love to hear your thoughts, comments, and recommendations along the way. Don’t hesitate! 😊</p>
</section>
<section id="why-now" class="level2">
<h2 class="anchored" data-anchor-id="why-now">Why Now? 🤷</h2>
<p>You know, I just realized that I never really took the time to properly introduce this blog or explain what it’s all about. Better late than never, right? 😁 So, here it is! 🎉</p>
</section>
<section id="how-its-built" class="level2">
<h2 class="anchored" data-anchor-id="how-its-built">How It’s Built 🛠️</h2>
<p>For all the techies out there, I’m using Quarto, and it’s deployed with GitHub Pages. I wanted to keep things simple but also have the flexibility to share code. Quarto’s been perfect for that – super interesting and easy to get the hang of. 👍</p>
</section>
<section id="what-else" class="level2">
<h2 class="anchored" data-anchor-id="what-else">What Else? 🧐</h2>
<p>Oh, and one more thing – I’ve added a page to showcase all the projects I’m working on, both finished and in progress. I’ll be publishing articles explaining them, sharing why I find them interesting, and how they’re coming along. Stay tuned! 🚧</p>
<p>Got any comments or ideas on how I can improve? I’m all ears! 👂 I’ll be updating regularly on my progress and any news, so keep an eye out for more.</p>
<p>And hey, I speak French too! Je peux bien parler en français qu’en anglais, permettez-moi ce petit écart 😉.</p>
</section>
<section id="ok-time-to-say-goodbye" class="level2">
<h2 class="anchored" data-anchor-id="ok-time-to-say-goodbye">Ok, Time to Say Goodbye 👋</h2>
<p>That’s all for now, folks! See you soon, and take care! Love you all! 💖</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a> ]]></description>
  <category>Personal Projects</category>
  <category>Updates</category>
  <guid>https://amenalahassa.github.io/amenalahassa/posts/for_you.html</guid>
  <pubDate>Fri, 23 Aug 2024 00:00:00 GMT</pubDate>
  <media:content url="https://amenalahassa.github.io/amenalahassa/posts/assets/images/0_0.png" medium="image" type="image/png" height="165" width="144"/>
</item>
<item>
  <title>Passer de Yolo à CocoDetection, c’est possible !</title>
  <link>https://amenalahassa.github.io/amenalahassa/posts/dataset-converter.html</link>
  <description><![CDATA[ 




<section id="de-quoi-sagit-il" class="level1">
<h1>De quoi s’agit-il ?</h1>
<p>Dans ce post, je montre comment convertir un dataset YOLO en format COCO en utilisant la bibliothèque Python <code>globox</code>, une solution pratique que j’ai récemment appliquée pour un projet de segmentation d’instance. <code>globox</code> permet également de faire l’inverse, ainsi que de nombreuses autres conversions de formats de datasets. Je vous invite à explorer ce package pour découvrir toutes les possibilités qu’il offre.</p>
</section>
<section id="ok-ça-donne-quoi" class="level1">
<h1>Ok, ça donne quoi ?</h1>
<div id="cell-4" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install globox</span>
<span id="cb1-2"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install pycocotools</span></code></pre></div>
</details>
</div>
<div id="cell-5" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pathlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Path</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> PIL <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Image</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torchvision.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CocoDetection</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> globox <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AnnotationSet</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.patches <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> patches</span>
<span id="cb2-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span></code></pre></div>
</details>
</div>
<p>Le code ci-dessous initialise trois chemins en utilisant la bibliothèque <code>Path</code> de Python. Les chemins <code>label_path</code> et <code>image_path</code> pointent respectivement vers les dossiers contenant les étiquettes et les images du dataset YOLO. Ensuite, le chemin <code>save_file</code> est défini pour sauvegarder le fichier de sortie au format COCO. Enfin, la commande <code>annotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path)</code> crée un ensemble d’annotations à partir des étiquettes YOLO et des images associées, prêt à être converti au format COCO.</p>
<div id="cell-7" class="cell" data-execution="{&quot;iopub.status.busy&quot;:&quot;2024-05-31T01:59:02.484254Z&quot;,&quot;iopub.execute_input&quot;:&quot;2024-05-31T01:59:02.484793Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-05-31T01:59:36.773381Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-05-31T01:59:02.484759Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-05-31T01:59:36.772245Z&quot;}" data-trusted="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">label_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/labels"</span>)</span>
<span id="cb3-2">image_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/images"</span>) </span>
<span id="cb3-3"></span>
<span id="cb3-4">save_file <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Path(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"/kaggle/working/medleaves-coco.json"</span>)</span>
<span id="cb3-5"></span>
<span id="cb3-6">annotations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AnnotationSet.from_yolo_v7(label_path, image_folder<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>image_path)</span></code></pre></div>
</div>
<p>La commande <code>annotations.show_stats()</code> est utilisée pour afficher des statistiques sur l’ensemble d’annotations créé précédemment. Cette méthode fournit un résumé utile des données annotées, comme le nombre total d’images, d’annotations, et la distribution des catégories d’objets. Cela permet de vérifier rapidement l’état et la qualité des annotations avant de procéder à des opérations de conversion ou d’entraînement de modèles.</p>
<div id="cell-9" class="cell" data-execution="{&quot;iopub.status.busy&quot;:&quot;2024-05-31T01:59:41.476666Z&quot;,&quot;iopub.execute_input&quot;:&quot;2024-05-31T01:59:41.477084Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-05-31T01:59:41.546606Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-05-31T01:59:41.477049Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-05-31T01:59:41.545284Z&quot;}" data-trusted="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">annotations.show_stats()</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-style: italic">      Database Stats      </span>
┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃<span style="font-weight: bold"> Label </span>┃<span style="font-weight: bold"> Images </span>┃<span style="font-weight: bold"> Boxes </span>┃
┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0     │    415 │  2322 │
│ 1     │    408 │   498 │
│ 10    │    416 │   588 │
│ 11    │    412 │   612 │
│ 12    │    414 │  2401 │
│ 13    │    410 │   810 │
│ 14    │    412 │  2750 │
│ 15    │    408 │  1605 │
│ 16    │    411 │  2237 │
│ 17    │    411 │  2662 │
│ 18    │    409 │   682 │
│ 19    │    413 │  5032 │
│ 2     │    414 │  5830 │
│ 20    │    405 │   882 │
│ 21    │    415 │  2892 │
│ 22    │    409 │   540 │
│ 23    │    414 │  1275 │
│ 24    │    411 │   727 │
│ 25    │    407 │  4778 │
│ 26    │    413 │  1255 │
│ 27    │    406 │   997 │
│ 28    │    411 │   618 │
│ 29    │    409 │  1143 │
│ 3     │    411 │  1584 │
│ 4     │    405 │  3082 │
│ 5     │    405 │  1275 │
│ 6     │    416 │   769 │
│ 7     │    403 │   570 │
│ 8     │    409 │  1563 │
│ 9     │    410 │  2475 │
├───────┼────────┼───────┤
│<span style="font-weight: bold"> Total </span>│<span style="font-weight: bold">  12312 </span>│<span style="font-weight: bold"> 54454 </span>│
└───────┴────────┴───────┘
</pre>
</div>
</div>
<p>C’est ici que la magie à lieu ! La commande <code>annotations.save_coco(save_file, auto_ids=True)</code> sauvegarde l’ensemble d’annotations dans un fichier au format COCO, en utilisant le chemin spécifié par <code>save_file</code>. L’option <code>auto_ids=True</code> indique que les identifiants pour les annotations et les images seront générés automatiquement si nécessaire.</p>
<div id="cell-11" class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">annotations.save_coco(save_file, auto_ids<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>Visualisons ensuite le résultat, mais en chargeant les données au format CocoDetection. Le code ci-dessous fait cela en utilisant la classe <code>CocoDetection</code> de la bibliothèque PyTorch.</p>
<p>Le paramètre <code>root</code> est défini avec le chemin vers le dossier des images (<code>image_path</code>), tandis que <code>annFile</code> spécifie le chemin du fichier d’annotations au format COCO (<code>save_file</code>).</p>
<p>Enfin, <code>train_data[0][0]</code> accède à la première image du dataset, afin ainsi de le visualiser.</p>
<div id="cell-13" class="cell" data-execution="{&quot;iopub.status.busy&quot;:&quot;2024-05-31T02:00:43.044327Z&quot;,&quot;iopub.execute_input&quot;:&quot;2024-05-31T02:00:43.044769Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-05-31T02:00:43.221923Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-05-31T02:00:43.044735Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-05-31T02:00:43.220768Z&quot;}" data-trusted="true" data-execution_count="12">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">train_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CocoDetection(root <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> image_path, annFile <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> save_file, transform <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb6-2">train_data[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>
<figure class="figure">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/dataset-converter_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Profitons pour visualiser une image du dataset avec ses annotations (bbox) superposées sur l’image.</p>
<div id="cell-15" class="cell" data-execution="{&quot;iopub.status.busy&quot;:&quot;2024-05-31T02:04:51.070855Z&quot;,&quot;iopub.execute_input&quot;:&quot;2024-05-31T02:04:51.071241Z&quot;,&quot;iopub.status.idle&quot;:&quot;2024-05-31T02:04:51.522270Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2024-05-31T02:04:51.071213Z&quot;,&quot;shell.execute_reply&quot;:&quot;2024-05-31T02:04:51.520708Z&quot;}" data-trusted="true" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Obtenir la première image et ses annotations</span></span>
<span id="cb7-2">image, annotations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_data[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb7-3"></span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convertir l'image PIL en tableau numpy pour l'affichage</span></span>
<span id="cb7-5">image_np <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(image)</span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Créer un graphique</span></span>
<span id="cb7-8">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-9">ax.imshow(image_np)</span>
<span id="cb7-10"></span>
<span id="cb7-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Tracer chaque boîte englobante</span></span>
<span id="cb7-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ann <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> annotations:</span>
<span id="cb7-13">    bbox <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ann[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bbox'</span>]</span>
<span id="cb7-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Le format de boîte englobante COCO est [x, y, largeur, hauteur]</span></span>
<span id="cb7-15">    rect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> patches.Rectangle((bbox[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], bbox[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]), bbox[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], bbox[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, edgecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r'</span>, facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>)</span>
<span id="cb7-16">    ax.add_patch(rect)</span>
<span id="cb7-17"></span>
<span id="cb7-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Afficher le graphique</span></span>
<span id="cb7-19">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://amenalahassa.github.io/amenalahassa/posts/dataset-converter_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>J’espère que cela vous sera utile. N’hésitez pas à me contacter si vous avez des questions ou des recommendations. À la prochaine ! :-)</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Retour au sommet</a> ]]></description>
  <category>Tips &amp; Tricks</category>
  <category>Tutorial</category>
  <guid>https://amenalahassa.github.io/amenalahassa/posts/dataset-converter.html</guid>
  <pubDate>Thu, 30 May 2024 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
