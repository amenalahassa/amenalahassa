[
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "hello.html#polar-axis",
    "href": "hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/dataset-converter.html",
    "href": "posts/dataset-converter.html",
    "title": "Passer de Yolo à CocoDetection, c’est possible !",
    "section": "",
    "text": "De quoi s’agit-il ?\nDans ce post, je montre comment convertir un dataset YOLO en format COCO en utilisant la bibliothèque Python globox, une solution pratique que j’ai récemment appliquée pour un projet de segmentation d’instance. Globox permet également de faire l’inverse, ainsi que de nombreuses autres conversions de formats de datasets. Je vous invite à explorer ce package pour découvrir toutes les possibilités qu’il offre.\n\n\nOk, ça donne quoi ?\n\n\nCode\n!pip install globox!pip install pycocotools\n\n\n\n\nCode\nfrom pathlib import Pathfrom PIL import Imagefrom torchvision.datasets import CocoDetectionfrom globox import AnnotationSetimport matplotlib.pyplot as pltimport matplotlib.patches as patchesimport numpy as np\n\n\nLe code ci-dessous initialise trois chemins en utilisant la bibliothèque Path de Python. Les chemins label_path et image_path pointent respectivement vers les dossiers contenant les étiquettes et les images du dataset YOLO. Ensuite, le chemin save_file est défini pour sauvegarder le fichier de sortie au format COCO. Enfin, la commande annotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path) crée un ensemble d’annotations à partir des étiquettes YOLO V7 et des images associées, prêt à être converti au format COCO.\n\nlabel_path = Path(\"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/labels\")\nimage_path = Path(\"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/images\") \n\nsave_file = Path(\"/kaggle/working/medleaves-coco.json\")\n\nannotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path)\n\nLa commande annotations.show_stats() est utilisée pour afficher des statistiques sur l’ensemble d’annotations créé précédemment. Cette méthode fournit un résumé utile des données annotées, comme le nombre total d’images, d’annotations, et la distribution des catégories d’objets. Cela permet de vérifier rapidement l’état et la qualité des annotations avant de procéder à des opérations de conversion ou d’entraînement de modèles.\n\nannotations.show_stats()\n\n      Database Stats      \n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ Label ┃ Images ┃ Boxes ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ 0     │    415 │  2322 │\n│ 1     │    408 │   498 │\n│ 10    │    416 │   588 │\n│ 11    │    412 │   612 │\n│ 12    │    414 │  2401 │\n│ 13    │    410 │   810 │\n│ 14    │    412 │  2750 │\n│ 15    │    408 │  1605 │\n│ 16    │    411 │  2237 │\n│ 17    │    411 │  2662 │\n│ 18    │    409 │   682 │\n│ 19    │    413 │  5032 │\n│ 2     │    414 │  5830 │\n│ 20    │    405 │   882 │\n│ 21    │    415 │  2892 │\n│ 22    │    409 │   540 │\n│ 23    │    414 │  1275 │\n│ 24    │    411 │   727 │\n│ 25    │    407 │  4778 │\n│ 26    │    413 │  1255 │\n│ 27    │    406 │   997 │\n│ 28    │    411 │   618 │\n│ 29    │    409 │  1143 │\n│ 3     │    411 │  1584 │\n│ 4     │    405 │  3082 │\n│ 5     │    405 │  1275 │\n│ 6     │    416 │   769 │\n│ 7     │    403 │   570 │\n│ 8     │    409 │  1563 │\n│ 9     │    410 │  2475 │\n├───────┼────────┼───────┤\n│ Total │  12312 │ 54454 │\n└───────┴────────┴───────┘\n\n\n\nC’est ici que la magie à lieu ! La commande annotations.save_coco(save_file, auto_ids=True) sauvegarde l’ensemble d’annotations dans un fichier au format COCO, en utilisant le chemin spécifié par save_file. L’option auto_ids=True indique que les identifiants pour les annotations et les images seront générés automatiquement si nécessaire. Cette étape est essentielle pour préparer les annotations dans un format standardisé, facilitant ainsi leur utilisation dans divers outils et frameworks de vision par ordinateur.\n\nannotations.save_coco(save_file, auto_ids=True)\n\nVisualisons ensuite le résultat, mais en chargeant les données au format CocoDetection. Le code ci-dessous charge les données d’entraînement en utilisant la classe CocoDetection de la bibliothèque PyTorch.\nLe paramètre root est défini avec le chemin vers le dossier des images (image_path), tandis que annFile spécifie le chemin du fichier d’annotations au format COCO (save_file).\nLe paramètre transform est mis à None, indiquant qu’aucune transformation supplémentaire n’est appliquée aux images pour le moment. Enfin, train_data[0][0] accède à la première image du dataset, afin ainsi de le visualiser.\n\ntrain_data = CocoDetection(root = image_path, annFile = save_file, transform = None)\ntrain_data[0][0]\n\n\n\n\n\n\n\n\nProfitons pour visualiser une image du dataset d’entraînement avec ses annotations (bbox) superposées sur l’image.\n\n\nCode\n# Obtenir la première image et ses annotationsimage, annotations = train_data[0]# Convertir l'image PIL en tableau numpy pour l'affichageimage_np = np.array(image)# Créer un graphiquefig, ax = plt.subplots(1)ax.imshow(image_np)# Tracer chaque boîte englobantefor ann in annotations:    bbox = ann['bbox']    # Le format de boîte englobante COCO est [x, y, largeur, hauteur]    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')    ax.add_patch(rect)# Afficher le graphiqueplt.show()\n\n\n\n\n\n\n\n\n\nJ’espère que cela vous sera utile. N’hésitez pas à me contacter si vous avez des questions ou des recommendations. À la prochaine ! :-)\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "",
    "text": "AI Developer fluent in machine learning and web technologies, I’m passionate about leveraging AI to tackle real-world problems and create a tangible impact. My primary interests lie in revolutionizing industries such as food production and finance through innovative technological solutions. With a deep commitment to sharing knowledge and fostering collaboration, I actively contribute to open-source projects and engage with the tech community.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#experiences",
    "href": "about.html#experiences",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Experiences",
    "text": "Experiences\n\nLead Developer | Sept 2022 - Jul 2023\nAgroSfer | Cotonou, Bénin\nLead development teams in creating a dynamic data aggregation system for enhanced dashboard analytics. Utilized Angular, Laravel, and MongoDB technologies.\n\n\nFront-end Developer | Jun 2021 - Sept 2022\nAgroSfer | Cotonou, Bénin\nStudied advanced software systems, focusing on practical and theoretical aspects of software engineering in Benin.\n\n\nInternship in Web Development | Mars 2021 - Mai 2021\nLa Vedette Media Digital | Cotonou, Bénin\nDesign and development of the architecture for an intranet-based project management web platform for La Vedette Media.\n\n\nInternship in Web Development | June 2020 - Sept 2020\nNAUTILUS TECHNOLOGY | Cotonou, Bénin\nInternship role focusing on web development using JavaScript, Angular, Laravel, and Git, solidifying foundational web development skills.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Education",
    "text": "Education\n\nMaîtrise en informatique - Intelligence Artificielle | Since Sept 2023\nUniversité Laval | Québec, CA\nPursuing a Master’s degree focusing on Artificial Intelligence, enhancing skills in complex AI systems and machine learning implementations.\n\n\nSystèmes informatiques et Logiciels | Oct 2018 - Jul 2021\nLes COURS SONOU | Abomey-Calavi, Bénin\nStudied advanced software systems, focusing on practical and theoretical aspects of software engineering in Benin.\n\n\nBaccalauréat Science Maths | Août 2018\nCEG Application | Porto-Novo, Bénin\nGraduated with a focus on Mathematics and Science, forming a strong analytical foundation.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Skills",
    "text": "Skills\n\nArtificial Intelligence\nPython, Pytorch, Tensorflow, Keras\nMatplotlib, Seaborn, Numpy, Pandas, timm, fastai, fastcore, torchvision, Poutyne,\nSPSS Modeler, RStudio\n\n\nSoftware engineering\nPHP, Javascript, C, C++, Java\nAngular, ReactJs, VueJs, Cypress, Web API\nSQL, Mysql, Firebase, MongoDB, Web Services, APIs Rest\n\n\nOthers\nGit, DevOps, CI/CD, UX Design, Scrum, Agile",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#online-courses",
    "href": "about.html#online-courses",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Online courses",
    "text": "Online courses\n\nFrom Engineer to Technical Manager: A Survival Guide, Sundog Education\nUdemy - 2023",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "multiverse",
    "section": "",
    "text": "Passer de Yolo à CocoDetection, c’est possible !\n\n\n\n\n\n\nYOLO\n\n\nCocoDetection\n\n\nDataset\n\n\n\nJe partage dans cet article une méthode simple pour convertir un dataset YOLO au format COCO, ou à un autre, une solution que j’ai trouvée essentielle lors d’un récent projet de segmentation d’instance.\n\n\n\n\n\n30 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting reply under comment with LLM\n\n\n\n\n\n\nLLM\n\n\nClassification\n\n\nUnbalanced Dataset\n\n\n\nFinetuning of Camembert on predicting, using a Le Soleil Facebook post comment as input, whether it will receive a response or not.\n\n\n\n\n\n21 mai 2024\n\n\n\n\n\n\nAucun article correspondant\n\n Retour au sommet",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html",
    "href": "posts/predicting_comment_reply_llm.html",
    "title": "Predicting reply under comment with LLM",
    "section": "",
    "text": "This project originally started as a school assignment for Big Data class. The notebook presented here demonstrates the use of a large language model (LLM) to tackle a binary classification problem. Specifically, our objective is to predict whether a comment will receive a response or not.\nTo achieve this, we use an enriched dataset compiled from comments on Le Soleil’s Facebook posts. I will also share a separate notebook detailing the process of building this dataset. Additionally, I plan to publish another post explaining how to utilize simple feedforward neural networks or statistical models based on various comment features or the comment text itself.\nLet’s dive in!"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#context",
    "href": "posts/predicting_comment_reply_llm.html#context",
    "title": "Predicting reply under comment with LLM",
    "section": "",
    "text": "This project originally started as a school assignment for Big Data class. The notebook presented here demonstrates the use of a large language model (LLM) to tackle a binary classification problem. Specifically, our objective is to predict whether a comment will receive a response or not.\nTo achieve this, we use an enriched dataset compiled from comments on Le Soleil’s Facebook posts. I will also share a separate notebook detailing the process of building this dataset. Additionally, I plan to publish another post explaining how to utilize simple feedforward neural networks or statistical models based on various comment features or the comment text itself.\nLet’s dive in!"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#dependence",
    "href": "posts/predicting_comment_reply_llm.html#dependence",
    "title": "Predicting reply under comment with LLM",
    "section": "Dependence",
    "text": "Dependence\nI prefer to set aside the cell that install system dependency. It always produces a lot of useless gx3di3ce… You get it, right ?\n\n\nCode\n!pip install torchsampler\n!pip install sacremoses\n\n\nNow, let’s import some packages to have fun !\n\n\nCode\nimport transformers, torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport multiprocessing as mp\nimport time\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom torchsampler import ImbalancedDatasetSampler\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom IPython.display import clear_output\n\n# warnings.filterwarnings('ignore')\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmp.cpu_count() # used to set the number of num_worker for Dataloader, usually the half of it \n\n\nAnd since I use Google colab, I mount my drive to load the datasets later.\n\n# only if you are using Google colab of course...\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#about-our-datasets",
    "href": "posts/predicting_comment_reply_llm.html#about-our-datasets",
    "title": "Predicting reply under comment with LLM",
    "section": "About our datasets",
    "text": "About our datasets\nIn this section, we will explore the dataset used for our binary classification problem. The dataset has been divided into training and testing sets, with 70% of the data allocated for training and the remaining 30% for testing. This split ensures that we have a robust training set to build our model while retaining a sufficient portion of the data for evaluating the model’s performance.\nLet’s load the train and the test sets.\n\ndirpath = '/content/drive/MyDrive/DataSets/big_data/datasets' # specify here the path to the dataset\ntrain = pd.read_csv(dirpath + '/split/train_dataset.csv', index_col=0)\ntest = pd.read_csv(dirpath + '/split/valid_dataset.csv', index_col=0)\n\nA few statistics about our dataset. First of all, it contains almost a million rows.\n\n\nCode\ndataset = pd.concat([train, test])\nprint(f'Dataset shape: {dataset.shape}')\n\n\nDataset shape: (935698, 68)\n\n\nSecondly, the dataset is highly unbalanced. The following graph shows that there are only ~13% of comments with replies, by which I mean that these comments have received at least one comment.\n\n\nCode\ndataset['target'].value_counts(normalize=True)\n\n\ntarget\nFalse    0.876036\nTrue     0.123964\nName: proportion, dtype: float64\n\n\n\n\nCode\ndataset['target'].value_counts().plot(kind='bar')\n\n\n\n\n\n\n\n\nFigure 1: Barchart of the count of each class\n\n\n\n\n\nWe have then two significant issues. Firstly, running the training on the entire dataset would be extremely time-consuming, even with substantial computational resources. Secondly, our dataset is unbalanced, which presents a challenge for accurate model training.\nTo address these issues, I opted for undersampling, ensuring an equal number of items from each class. This approach allows us to run our experiments more efficiently and mitigates the problem of data imbalance. Later in the notebook, we will explore the impact of the amount of data used on the model’s performance."
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#class-and-functions",
    "href": "posts/predicting_comment_reply_llm.html#class-and-functions",
    "title": "Predicting reply under comment with LLM",
    "section": "Class and functions",
    "text": "Class and functions\nI write the CommentDataset class as a custom dataset designed for handling text data. It inherits from the Dataset class provided by PyTorch. This class is specifically tailored for tokenizing and preparing text data along with their corresponding labels for use in a model.\n\n\nCode\nclass CommentDataset(Dataset):\n    def __init__(self, message, labels, tokenizer):\n        self.message = message\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def get_labels(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.message)\n\n    def __getitem__(self, idx):\n        text = self.message[idx]\n        label = self.labels[idx]\n\n        inputs = self.tokenizer.encode_plus(text, None, add_special_tokens=True, padding='max_length', return_token_type_ids=True, truncation=True)\n\n        return {\n            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            'labels': torch.tensor(label, dtype=torch.float)\n        }\n\n\nThe train_model method is designed to train a machine learning model using a provided training and testing dataloader, while tracking various performance metrics such as loss, accuracy, precision, recall, and a custom F2 score across multiple epochs, and implementing early stopping based on validation performance. The test_model method evaluates the trained model on a validation dataset, computing and printing evaluation metrics to assess the model’s performance.\n\n\nCode\ndef train_model(model, train_dataloader, test_dataloader, history={}, num_epochs=5, lr=5e-5, early_stopping_patience=3, weight_decay=0.01):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1)  # ReduceLROnPlateau scheduler\n    loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n\n    history['train_loss'] = []\n    history['train_accuracy'] = []\n    history['train_precision'] = []\n    history['train_recall'] = []\n    history['test_accuracy'] = []\n    history['test_precision'] = []\n    history['test_recall'] = []\n    history['epochs'] = []\n    history['test_loss'] = []\n    history['valid_score'] = []\n    best_valid_score = 0\n    early_stopping_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_preds = []\n        train_labels = []\n\n        # Training loop\n        for _, batch in enumerate(tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}')):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            logits = outputs.logits.squeeze(1)\n            loss = loss_fn(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_preds.extend((logits &gt; 0.5).int().tolist())\n            train_labels.extend(labels.tolist())\n\n        # Calculate metrics on training set\n        train_accuracy = accuracy_score(train_labels, train_preds)\n        train_precision = precision_score(train_labels, train_preds, average='binary')\n        train_recall = recall_score(train_labels, train_preds, average='binary')\n\n        # Evaluation loop\n        model.eval()\n        test_preds = []\n        test_labels = []\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for batch in test_dataloader:\n\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                token_type_ids = batch['token_type_ids'].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                logits = outputs.logits.squeeze(1)\n                loss = loss_fn(logits, labels)\n\n                test_loss += loss.item()\n                test_preds.extend((logits &gt; 0.5).int().tolist())\n                test_labels.extend(labels.tolist())\n\n        # Calculate metrics on test set\n        test_accuracy = accuracy_score(test_labels, test_preds)\n        test_precision = precision_score(test_labels, test_preds, average='binary')\n        test_recall = recall_score(test_labels, test_preds, average='binary')\n        tn, fp, fn, tp = confusion_matrix(test_labels, test_preds).ravel()\n        valid_score = (tp / (tp + fp + fn)) * 100\n\n        # Update learning rate scheduler\n        scheduler.step(valid_score)\n\n        history['epochs'].append(epoch + 1)\n\n        history['train_loss'].append(train_loss / len(train_dataloader))\n        history['train_accuracy'].append(train_accuracy)\n        history['train_precision'].append(train_precision)\n        history['train_recall'].append(train_recall)\n\n        history['test_loss'].append(test_loss / len(test_dataloader))\n        history['test_accuracy'].append(test_accuracy)\n        history['test_precision'].append(test_precision)\n        history['test_recall'].append(test_recall)\n        history['valid_score'].append(valid_score)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n\n        print(f\"  Train Loss: {train_loss / len(train_dataloader)}\")\n        print(f\"  Test Loss: {test_loss / len(test_dataloader)}\")\n        print(f\"  Train Accuracy: {train_accuracy}\")\n        print(f\"  Train Precision: {train_precision}\")\n        print(f\"  Train Recall: {train_recall}\")\n\n        print(f\"  Test Accuracy: {test_accuracy}\")\n        print(f\"  Test Precision: {test_precision}\")\n        print(f\"  Test Recall: {test_recall}\")\n        print(f\"  Test F2: {valid_score}\")\n\n        # Early stopping\n        if valid_score &gt; best_valid_score:\n            best_valid_score = valid_score\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n\n        if early_stopping_counter &gt;= early_stopping_patience:\n            print(\"Early stopping triggered!\")\n            break\n\ndef test_model(tokz, model, valid_data, history, device, bs = 16):\n    model.eval()\n    test_preds = []\n    test_labels = []\n    test_loss = 0.0\n\n    valid_dataset = CommentDataset(valid_data[0].to_numpy(), valid_data[1].astype(int).to_numpy(), tokz)\n    test_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=bs, shuffle=True)\n    loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            logits = outputs.logits.squeeze(1)\n            loss = loss_fn(logits, labels)\n\n            test_loss += loss.item()\n            test_preds.extend((logits &gt; 0.5).int().tolist())\n            test_labels.extend(labels.tolist())\n\n    test_accuracy = accuracy_score(test_labels, test_preds)\n    test_precision = precision_score(test_labels, test_preds)\n    test_recall = recall_score(test_labels, test_preds)\n    tn, fp, fn, tp = confusion_matrix(test_labels, test_preds).ravel()\n    history['valid_score'] = (tp / (tp + fp + fn)) * 100\n\n    print(\"Test Metrics:\")\n    print(f\"  Eval Accuracy: {test_accuracy}\")\n    print(f\"  Eval Precision: {test_precision}\")\n    print(f\"  Eval Recall: {test_recall}\")\n    print(f\"  Eval F2: {history['valid_score']}\")\n\n\nThe plot_history method visualizes the training and testing metrics (loss, accuracy, precision, recall, and F2 score) over epochs using matplotlib. The evaluate_model function assesses the model by optionally plotting the training history and running the test_model function for evaluation metrics. The get_loader function prepares the data loaders for training and testing datasets, including optional under-sampling, and sets up the tokenizer and model for sequence classification tasks. The equal_class_sampling method ensures balanced class distribution by sampling an equal number of instances from each class in the dataset.\n\n\nCode\ndef plot_history(history):\n    plt.figure(figsize=(17, 6))\n\n    epochs = history['epochs']\n    train_losses = history['train_loss']\n    test_loss = history['test_loss']\n    train_accuracies = history['train_accuracy']\n    test_accuracies = history['test_accuracy']\n    train_precisions = history['train_precision']\n    test_precisions = history['test_precision']\n    train_recall = history['train_recall']\n    test_recall = history['test_recall']\n    valid_score = history['valid_score']\n\n    plt.subplot(1, 5, 1)\n    plt.plot(epochs, train_losses, label='Training Loss')\n    plt.plot(epochs, test_loss, label='Test Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n\n    plt.subplot(1, 5, 2)\n    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 5, 3)\n    plt.plot(epochs, train_precisions, label='Training Precision')\n    plt.plot(epochs, test_precisions, label='Test Precision')\n    plt.xlabel('Epochs')\n    plt.ylabel('Precision')\n    plt.title('Precision')\n    plt.legend()\n\n    plt.subplot(1, 5, 4)\n    plt.plot(epochs, train_recall, label='Training Recall')\n    plt.plot(epochs, test_recall, label='Test Recall')\n    plt.xlabel('Epochs')\n    plt.ylabel('Recall')\n    plt.title('Recall')\n    plt.legend()\n\n    plt.subplot(1, 5, 5)\n    plt.plot(epochs, valid_score, label='Training F2')\n    plt.xlabel('Epochs')\n    plt.ylabel('F2')\n    plt.title('F2')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n    \ndef evaluate_model(tokz, model, valid_data, history, device, bs = 16, plot_train=True):\n    if plot_train:\n        plot_history(history)\n    test_model(tokz, model, valid_data, history, device, bs = bs)\n    \ndef get_loader(model_nm, dataset, bs = 100, under_sample=True, num_class = 1, use_pad_token=True, use_special_pad_token=False, num_workers=2):\n    X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']\n    tokz = AutoTokenizer.from_pretrained(model_nm)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels = num_class)\n\n    if len(X_train) == 0:\n      return model, tokz, None, None\n\n    if use_pad_token:\n        tokz.pad_token = tokz.eos_token\n    if use_special_pad_token:\n        tokz.add_special_tokens({'pad_token': '[PAD]'})\n\n    model.resize_token_embeddings(len(tokz))\n\n    train_dataset = CommentDataset(X_train.to_numpy(), y_train.astype(int).to_numpy(), tokz)\n    test_dataset = CommentDataset(X_test.to_numpy(), y_test.astype(int).to_numpy(), tokz)\n\n    if under_sample:\n        train_loader = torch.utils.data.DataLoader(train_dataset, sampler=ImbalancedDatasetSampler(train_dataset), batch_size=bs, num_workers=num_workers, pin_memory=True)\n        test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=bs, num_workers=num_workers, pin_memory=True)\n    else:\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n\n    return model, tokz, train_loader, test_loader\n\ndef equal_class_sampling(input_features, target_labels, num_samples):\n    num_classes = len(target_labels.unique())\n    num_samples_per_class = num_samples // num_classes\n    dataset = pd.DataFrame({'input': input_features, 'target': target_labels})\n    grouped = dataset.groupby(['target'])\n    sampled_elements = grouped.apply(lambda x: x.sample(min(num_samples_per_class, len(x))))\n    return sampled_elements['input'], sampled_elements['target']"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#training",
    "href": "posts/predicting_comment_reply_llm.html#training",
    "title": "Predicting reply under comment with LLM",
    "section": "Training",
    "text": "Training\n\nEvaluations\nTo evaluate our model, we will primarily use recall and a custom metric that we will call F2. Recall measures the ability of the model to correctly identify all relevant instances (true positives) from the dataset and is calculated as TP/(TP + FN), where TP stands for true positives and FN stands for false negatives.\nThe custom metric, F2, is designed to provide a more comprehensive evaluation of the model’s performance by balancing the detection of the positive class and minimizing errors. The F2 score is calculated as TP/(TP + FN + FP), where FP stands for false positives. This metric helps evaluate the model’s capacity to detect the positive class correctly while accounting for both false negatives and false positives. By considering both types of errors, the F2 metric ensures that the model is not only identifying positive instances accurately but also minimizing the incorrect classification of negative instances as positive. This balanced approach provides a more nuanced assessment of the model’s overall effectiveness.\n\n\n\nModeling\nFor simplicity’s sake, I’ll use the distill version of Camembert model here, but you’re free to use any of the models below.\n\nmodels = {\n    'bert': \"bert-base-uncased\",\n    'gpt': \"distilgpt2\",\n    'flau': \"flaubert/flaubert_base_uncased\",\n    'cmb': \"cmarkea/distilcamembert-base\",\n}\n\n\n\nFine tuning\nThe next code might look a bit confusing at first, but let’s break it down step by step.\nWhat we’re doing here is creating smaller subsets from our original training and test sets to build training, validation and test samples.\n\n# Grab the 'message' and 'target' columns from the training set and store them in X_train and y_train\nX_train, y_train = train['message'], train['target']\n\nNext, we split off a small portion of the original test set to use as our validation set. This is like keeping a small piece of pie aside before sharing the rest.\n\nX_valid_sample, X_valid, y_valid_sample, y_valid = train_test_split(test['message'], test['target'], test_size=0.95, random_state=42)\nX_valid_sample.shape, X_valid.shape, y_valid_sample.shape, y_valid.shape\n\n((9357,), (177783,), (9357,), (177783,))\n\n\nThen, we balance our training data. Imagine we have 6000 rows, and we want to make sure we have an equal number of positive and negative samples—3000 of each.\n\nX_train_sample, y_train_sample = equal_class_sampling(X_train, y_train, 6000)\n\nFinally, we take another small slice of the test set to build our final test sample. Think of this as taking a tiny bit more of that pie for a taste test.\n\n# Split the remaining validation set to create a small test sample (2% of X_valid and y_valid)\n_, X_test_sample, _, y_test_sample = train_test_split(X_valid, y_valid, test_size=0.02, random_state=42)\nX_test_sample.shape\n\n(3556,)\n\n\nBy doing this, we ensure our model has balanced and representative data for training, validation, and final testing.\nNow, we train, we validate and evaluate the model on the test set.\n\n\nCode\nhistory = {}  # Initialize an empty dictionary to store training and evaluation history.\nBATCH_SIZE = 16  # Define the batch size for data loaders.\nLEARNING_RATE = 1e-4  # Set the learning rate for the optimizer.\nweight_decay = 1e-2  # Set the weight decay (L2 regularization) for the optimizer.\nEPOCHS = 10  # Set the number of epochs for training.\n\n# Prepare the dataset dictionary with training and testing samples.\ndata = {'X_train': X_train_sample, 'y_train': y_train_sample, 'X_test': X_test_sample, 'y_test': y_test_sample}\n\n# Get the model, tokenizer, training data loader, and testing data loader.\nmodel, tokz, train_loader, test_loader = get_loader(models['cmb'], data, bs=BATCH_SIZE, use_special_pad_token=True, num_workers=8)\n\nmodel.to(device)  # Move the model to the specified device (CPU or GPU).\n\nstart_time = time.time()  # Record the start time for training.\ntrain_model(model, train_loader, test_loader, history, num_epochs=EPOCHS, lr=LEARNING_RATE, early_stopping_patience=2, weight_decay=weight_decay)  # Train the model.\nend_time = time.time()  # Record the end time for training.\nexecution_time = end_time - start_time  # Calculate the execution time for training.\n\nclear_output()  # Clear the output (useful in Jupyter notebooks to clear previous outputs).\nprint(\"Execution time:\", execution_time, \"seconds\")  # Print the execution time for training.\n\nstart_time = time.time()  # Record the start time for evaluation.\n# Evaluate the model on the validation dataset and optionally plot the training history.\nevaluate_model(tokz, model, (X_valid_sample, y_valid_sample), history, device, bs = BATCH_SIZE * 2, plot_train=True)\nend_time = time.time()  # Record the end time for evaluation.\nexecution_time = end_time - start_time  # Calculate the execution time for evaluation.\nprint(\"Execution time:\", execution_time, \"seconds\")  # Print the execution time for evaluation.\n\n\nExecution time: 363.3319444656372 seconds\n\n\n\n\n\n\n\n\nFigure 2: Series of graphs depicting the performance metrics of the model. The metrics include Training Loss, Accuracy, Precision, Recall, and F2 score for both training and testing data.\n\n\n\n\n\nTest Metrics:\n  Eval Accuracy: 0.6475366036122688\n  Eval Precision: 0.23312101910828026\n  Eval Recall: 0.7605985037406484\n  Eval F2: 21.7184903868977\nExecution time: 84.7105667591095 seconds\n\n\nBased on the performance of the model on two epochs, we can make the following analysis:\n\nTraining Loss: Decreases from approximately 0.55 to 0.45, indicating improved performance during training.\nAccuracy: Shows a significant increase for training accuracy from about 0.70 to 0.78, while test accuracy slightly improves from around 0.62 to 0.64.\nPrecision: Training precision rises from about 0.75 to 0.80, whereas test precision remains almost constant at around 0.21.\nRecall: Training recall increases from 0.60 to 0.75, while test recall decreases from 0.85 to 0.70, suggesting potential overfitting.\nF2: Indicates a decline in the F2 score from approximately 21.2 to 20.2, which means that the model is not generalizing from the training data to the test data.\n\n\nTest Metrics:\n\nEval Accuracy: 0.6475, which indicates the model’s ability to correctly predict test data is moderate but lower compared to training accuracy.\nEval Precision: 0.2331, which is significantly lower than the training precision, suggesting the model struggles with false positives on the test set.\nEval Recall: 0.7606, which is relatively high and close to the training recall, showing the model still performs well in identifying most positive instances on the test set.\nEval F2: 21.7185, which remains high, indicating that despite high recall, the model struggles with false positives.\n\n\n\nConclusion:\n\nOverfitting: The discrepancy between training and test precision suggests overfitting. The model performs well on the training data but struggles with generalization, leading to lower performance on unseen data.\nF2 score: The model prioritizes recall over precision. This is evident from the high recall but low precision on the test set. This behavior is further reflected in the F2 score, which is low, indicating many false positives.\n\nThis analysis suggest that there is a problem with our model, because we need a model that should perform well on unseen data with low errors.\n\n\nWhy is this important?\nWell, imagine that we will deploy our model in a real-world application. We don’t want to miss comments that might receive a response because we could use them to increase traffic on our site or social media. In that case, a model that detects positive instances well with minimal false positives is acceptable. However, our model currently has many false positives, which can be problematic.\n\n\nPractical Implications:\n\nBusiness Impact: If the model is used in an application like content moderation or customer feedback analysis, high false positives mean that many irrelevant comments would be flagged for response. This can lead to inefficient use of resources and missed opportunities to engage with truly relevant comments.\nUser Experience: In applications like spam detection, a high number of false positives can frustrate users, as legitimate messages may be incorrectly flagged as spam.\nOperational Efficiency: For customer service applications, responding to false positives wastes time and effort that could be better spent addressing genuine issues.\n\nBut, let’s try with more data in our training set to see the impact on the model performance. We will initialise a new model and train it on 10000 comments.\n\n\nCode\nX_train, y_train = train['message'], train['target']\nX_train_sample, y_train_sample = equal_class_sampling(X_train, y_train, 10000)\n_, X_test_sample, _, y_test_sample = train_test_split(X_valid, y_valid, test_size=0.02, random_state=42)\n\n\n\n\nCode\nhistory = {}\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-4\nweight_decay = 1e-2\nEPOCHS = 10\ndata = {'X_train': X_train_sample, 'y_train': y_train_sample, 'X_test': X_test_sample, 'y_test': y_test_sample}\nmodel, tokz, train_loader, test_loader = get_loader(models['cmb'], data, bs=BATCH_SIZE, use_special_pad_token=True, num_workers=8)\nmodel.to(device)\n\nstart_time = time.time()\ntrain_model(model, train_loader, test_loader, history, num_epochs=EPOCHS, lr=LEARNING_RATE, early_stopping_patience=2, weight_decay=weight_decay)\nend_time = time.time()\nexecution_time = end_time - start_time\n\nclear_output()\nprint(\"Execution time:\", execution_time, \"seconds\")\n\nstart_time = time.time()\nevaluate_model(tokz, model, (X_valid_sample, y_valid_sample), history, device, bs = BATCH_SIZE * 2, plot_train=True)\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(\"Execution time:\", execution_time, \"seconds\")\n\n\nExecution time: 1122.6102643013 seconds\n\n\n\n\n\n\n\n\nFigure 3: Series of graphs depicting the performance metrics of the model. The metrics include Training Loss, Accuracy, Precision, Recall, and F2 score for both training and testing data.\n\n\n\n\n\nTest Metrics:\n  Eval Accuracy: 0.6856898578604254\n  Eval Precision: 0.25793871866295265\n  Eval Recall: 0.769742310889443\n  Eval F2: 23.946211533488494\nExecution time: 84.70709776878357 seconds\n\n\nThe F2 score improved from 21% to 23%. But can we conclude that it’s the increased training set that induces these results?\nLet’s try it with a larger training set.\n\n\nCode\nX_train, y_train = train['message'], train['target']\nX_train_sample, y_train_sample = equal_class_sampling(X_train, y_train, 15000)\n_, X_test_sample, _, y_test_sample = train_test_split(X_valid, y_valid, test_size=0.02, random_state=42)\n\n\n\n\nCode\nhistory = {}\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-4\nweight_decay = 1e-2\nEPOCHS = 10\ndata = {'X_train': X_train_sample, 'y_train': y_train_sample, 'X_test': X_test_sample, 'y_test': y_test_sample}\nmodel, tokz, train_loader, test_loader = get_loader(models['cmb'], data, bs=BATCH_SIZE, use_special_pad_token=True, num_workers=8)\nmodel.to(device)\n\nstart_time = time.time()\ntrain_model(model, train_loader, test_loader, history, num_epochs=EPOCHS, lr=LEARNING_RATE, early_stopping_patience=2, weight_decay=weight_decay)\nend_time = time.time()\nexecution_time = end_time - start_time\n\nclear_output()\nprint(\"Execution time:\", execution_time, \"seconds\")\n\nstart_time = time.time()\nevaluate_model(tokz, model, (X_valid_sample, y_valid_sample), history, device, bs = BATCH_SIZE * 2, plot_train=True)\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(\"Execution time:\", execution_time, \"seconds\")\n\n\nExecution time: 1914.5363965034485 seconds\n\n\n\n\n\n\n\n\nFigure 4: Series of graphs depicting the performance metrics of the model. The metrics include Training Loss, Accuracy, Precision, Recall, and F2 score for both training and testing data.\n\n\n\n\n\nTest Metrics:\n  Eval Accuracy: 0.7079192048733568\n  Eval Precision: 0.26705237515225333\n  Eval Recall: 0.7290108063175395\n  Eval F2: 24.293628808864266\nExecution time: 85.33442664146423 seconds\n\n\nOn test set, the F2 score of the model improve again from 23% to 24%.\n\n\nConclusion:\nThe increase in the training set size helps the model generalize better to the test data, reducing overfitting and improving its ability to balance precision and recall effectively."
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#takeaways",
    "href": "posts/predicting_comment_reply_llm.html#takeaways",
    "title": "Predicting reply under comment with LLM",
    "section": "Takeaways",
    "text": "Takeaways\n\nDataset Analysis is Crucial: Always begin by analyzing your dataset. Understanding the distribution and characteristics of your data helps in making informed decisions about model training and evaluation. In scenarios with limited resources, generating more data for the underrepresented class might not be feasible. Instead, sampling equal numbers of comments from each class for the training set can help reduce bias towards the overrepresented class.\nBalanced Training, Unbalanced Validation: While balancing the training set by equal sampling is important to reduce bias, the validation set should remain unbalanced. This approach ensures that the model’s performance is evaluated in a realistic manner, reflecting its ability to generalize to the true distribution of the data.\nResource-Based Training Strategy: Define your training strategy based on the available resources. When computational power or time is limited, working with a smaller, balanced sample of the dataset is a practical approach. This allows for iterative experimentation and tuning without the overhead of processing the entire dataset.\nProblem-Specific Metrics: Choose evaluation metrics that align with your problem’s objectives. For instance, in this scenario, the F2 score (F2 = tp / (tp + 2 * fn + fp)) is used to evaluate model performance by balancing the detection of the positive class and minimizing errors.\nInitial Model Performance: After the first round of training, the F2 score indicates that the model prioritizes recall over precision. This is evidenced by the high recall but low precision on the test set.\nImpact of False Positives: High false positives can be problematic in real-world applications. They can lead to inefficient use of resources and missed opportunities to engage with truly relevant comments. This highlights the need for a balance between precision and recall.\nTraining Set Size and Generalization: Increasing the size of the training set helps the model generalize better to the test data. A larger training set reduces overfitting and enhances the model’s ability to balance precision and recall effectively. This results in improved overall performance and more reliable predictions.\nChoosing the Right Model: Select a model that is suitable for your specific problem. For instance, since the dataset consists of French text, using CamemBERT, a model specifically designed for the French language, is an appropriate choice.\nHyperparameter Tuning: Finding the optimal hyperparameters for your model is crucial and often involves extensive experimentation. Before finalizing the model, numerous combinations were tested to identify the best-performing configuration. Hyperparameter tuning is more of an art than a strict recipe, requiring intuition and experience to achieve the best results."
  }
]