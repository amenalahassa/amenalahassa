[
  {
    "objectID": "posts/dataset-converter.html",
    "href": "posts/dataset-converter.html",
    "title": "Passer de Yolo √† CocoDetection, c‚Äôest possible !",
    "section": "",
    "text": "De quoi s‚Äôagit-il ?\nDans ce post, je montre comment convertir un dataset YOLO en format COCO en utilisant la biblioth√®que Python globox, une solution pratique que j‚Äôai r√©cemment appliqu√©e pour un projet de segmentation d‚Äôinstance. globox permet √©galement de faire l‚Äôinverse, ainsi que de nombreuses autres conversions de formats de datasets. Je vous invite √† explorer ce package pour d√©couvrir toutes les possibilit√©s qu‚Äôil offre.\n\n\nOk, √ßa donne quoi ?\n\n\nCode\n!pip install globox\n!pip install pycocotools\n\n\n\n\nCode\nfrom pathlib import Path\nfrom PIL import Image\nfrom torchvision.datasets import CocoDetection\nfrom globox import AnnotationSet\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\n\n\nLe code ci-dessous initialise trois chemins en utilisant la biblioth√®que Path de Python. Les chemins label_path et image_path pointent respectivement vers les dossiers contenant les √©tiquettes et les images du dataset YOLO. Ensuite, le chemin save_file est d√©fini pour sauvegarder le fichier de sortie au format COCO. Enfin, la commande annotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path) cr√©e un ensemble d‚Äôannotations √† partir des √©tiquettes YOLO et des images associ√©es, pr√™t √† √™tre converti au format COCO.\n\nlabel_path = Path(\"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/labels\")\nimage_path = Path(\"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/images\") \n\nsave_file = Path(\"/kaggle/working/medleaves-coco.json\")\n\nannotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path)\n\nLa commande annotations.show_stats() est utilis√©e pour afficher des statistiques sur l‚Äôensemble d‚Äôannotations cr√©√© pr√©c√©demment. Cette m√©thode fournit un r√©sum√© utile des donn√©es annot√©es, comme le nombre total d‚Äôimages, d‚Äôannotations, et la distribution des cat√©gories d‚Äôobjets. Cela permet de v√©rifier rapidement l‚Äô√©tat et la qualit√© des annotations avant de proc√©der √† des op√©rations de conversion ou d‚Äôentra√Ænement de mod√®les.\n\nannotations.show_stats()\n\n      Database Stats      \n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ Label ‚îÉ Images ‚îÉ Boxes ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ 0     ‚îÇ    415 ‚îÇ  2322 ‚îÇ\n‚îÇ 1     ‚îÇ    408 ‚îÇ   498 ‚îÇ\n‚îÇ 10    ‚îÇ    416 ‚îÇ   588 ‚îÇ\n‚îÇ 11    ‚îÇ    412 ‚îÇ   612 ‚îÇ\n‚îÇ 12    ‚îÇ    414 ‚îÇ  2401 ‚îÇ\n‚îÇ 13    ‚îÇ    410 ‚îÇ   810 ‚îÇ\n‚îÇ 14    ‚îÇ    412 ‚îÇ  2750 ‚îÇ\n‚îÇ 15    ‚îÇ    408 ‚îÇ  1605 ‚îÇ\n‚îÇ 16    ‚îÇ    411 ‚îÇ  2237 ‚îÇ\n‚îÇ 17    ‚îÇ    411 ‚îÇ  2662 ‚îÇ\n‚îÇ 18    ‚îÇ    409 ‚îÇ   682 ‚îÇ\n‚îÇ 19    ‚îÇ    413 ‚îÇ  5032 ‚îÇ\n‚îÇ 2     ‚îÇ    414 ‚îÇ  5830 ‚îÇ\n‚îÇ 20    ‚îÇ    405 ‚îÇ   882 ‚îÇ\n‚îÇ 21    ‚îÇ    415 ‚îÇ  2892 ‚îÇ\n‚îÇ 22    ‚îÇ    409 ‚îÇ   540 ‚îÇ\n‚îÇ 23    ‚îÇ    414 ‚îÇ  1275 ‚îÇ\n‚îÇ 24    ‚îÇ    411 ‚îÇ   727 ‚îÇ\n‚îÇ 25    ‚îÇ    407 ‚îÇ  4778 ‚îÇ\n‚îÇ 26    ‚îÇ    413 ‚îÇ  1255 ‚îÇ\n‚îÇ 27    ‚îÇ    406 ‚îÇ   997 ‚îÇ\n‚îÇ 28    ‚îÇ    411 ‚îÇ   618 ‚îÇ\n‚îÇ 29    ‚îÇ    409 ‚îÇ  1143 ‚îÇ\n‚îÇ 3     ‚îÇ    411 ‚îÇ  1584 ‚îÇ\n‚îÇ 4     ‚îÇ    405 ‚îÇ  3082 ‚îÇ\n‚îÇ 5     ‚îÇ    405 ‚îÇ  1275 ‚îÇ\n‚îÇ 6     ‚îÇ    416 ‚îÇ   769 ‚îÇ\n‚îÇ 7     ‚îÇ    403 ‚îÇ   570 ‚îÇ\n‚îÇ 8     ‚îÇ    409 ‚îÇ  1563 ‚îÇ\n‚îÇ 9     ‚îÇ    410 ‚îÇ  2475 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ Total ‚îÇ  12312 ‚îÇ 54454 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\nC‚Äôest ici que la magie √† lieu ! La commande annotations.save_coco(save_file, auto_ids=True) sauvegarde l‚Äôensemble d‚Äôannotations dans un fichier au format COCO, en utilisant le chemin sp√©cifi√© par save_file. L‚Äôoption auto_ids=True indique que les identifiants pour les annotations et les images seront g√©n√©r√©s automatiquement si n√©cessaire.\n\nannotations.save_coco(save_file, auto_ids=True)\n\nVisualisons ensuite le r√©sultat, mais en chargeant les donn√©es au format CocoDetection. Le code ci-dessous fait cela en utilisant la classe CocoDetection de la biblioth√®que PyTorch.\nLe param√®tre root est d√©fini avec le chemin vers le dossier des images (image_path), tandis que annFile sp√©cifie le chemin du fichier d‚Äôannotations au format COCO (save_file).\nEnfin, train_data[0][0] acc√®de √† la premi√®re image du dataset, afin ainsi de le visualiser.\n\ntrain_data = CocoDetection(root = image_path, annFile = save_file, transform = None)\ntrain_data[0][0]\n\n\n\n\n\n\n\n\nProfitons pour visualiser une image du dataset avec ses annotations (bbox) superpos√©es sur l‚Äôimage.\n\n\nCode\n# Obtenir la premi√®re image et ses annotations\nimage, annotations = train_data[0]\n\n# Convertir l'image PIL en tableau numpy pour l'affichage\nimage_np = np.array(image)\n\n# Cr√©er un graphique\nfig, ax = plt.subplots(1)\nax.imshow(image_np)\n\n# Tracer chaque bo√Æte englobante\nfor ann in annotations:\n    bbox = ann['bbox']\n    # Le format de bo√Æte englobante COCO est [x, y, largeur, hauteur]\n    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\nJ‚Äôesp√®re que cela vous sera utile. N‚Äôh√©sitez pas √† me contacter si vous avez des questions ou des recommendations. √Ä la prochaine ! :-)\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/model_gpu.html",
    "href": "posts/model_gpu.html",
    "title": "How Much Memory Does Your Model Need on GPU? Let‚Äôs Find Out!",
    "section": "",
    "text": "A Tool to Estimate Model Memory Usage on GPU\nHey there, fellow developers and curious minds! üññ Ever wondered how much juice you need to run a model on your GPU? Whether you‚Äôre knee-deep in code or just dipping your toes into the tech waters, the question of resources has likely crossed your mind.\n\nBuilding a Tool to Estimate GPU Memory Usage\nGuess what? I built a tool for that! üéâ But hold up, before you get too excited‚Ä¶ it only estimates the memory required to load the model onto a GPU. Yep, just that‚Äîno more, no less. If you‚Äôre itching to test it out, here‚Äôs the link.\n\n\nWhy Only Estimate GPU Memory for Loading?\nYou might be asking, ‚ÄúWhy just the memory for loading?‚Äù Good question! It‚Äôs because to really figure out how much resources a model needs to run (whether training or inference), you actually need to run it. Yep, no shortcuts here.\nIt all depends on things like the size of your input/batch, the quality of your data (low-res images vs.¬†high-res by eg), and a whole bunch of other factors. ü§Ø Oh, and don‚Äôt forget the extra memory needed to store activations during the forward pass and gradients during the backward pass. The bigger the model, the more resources you‚Äôll need. It‚Äôs a classic case of go big or go home!\n\n\nSo, How Do I Know How Much Memory My Model Needs?\nHere‚Äôs the deal: The memory a model uses for training is not the same as it uses for inference. Training requires more memory because it needs to store activations and gradients for backpropagation. Unfortunately, there‚Äôs no one-size-fits-all rule to tell you exactly how much memory you‚Äôll need, but we can make an educated guess.\nYou can start by using my tool to get a minimum memory estimate for loading the model. To estimate training memory, a rough approach is to multiply that number by 10, but keep in mind this is just a ballpark figure. The exact factor can vary depending on your model architecture and batch size. To be safe, always test with different configurations to find the sweet spot.\nDuring inference, models generally require less memory and run faster compared to training, but there are exceptions. Monitoring tools like nvidia-smi can help you keep track of actual GPU usage and avoid surprises. üìä\n\n\nExample Time! üéÆ\nLet‚Äôs take a practical example. I want to fine-tune the Qwen 2 model (the smallest version) on a GPU P100 with 16GB of memory on Kaggle. When I load the model onto the GPU, it takes up around 2GB of memory. Not too bad, right? But when I try to train the model, you see in the image below, it maxes out all the resources. üöÄ\nAnd when it‚Äôs time for evaluation (where the model only does inference, no training), it crashes due to a lack of memory. ü§¶‚Äç‚ôÇÔ∏è Moral of the story: Even if you have the best GPU on the market, if you don‚Äôt have enough memory, you‚Äôre done for!\nIn fact, if I increase the batch size or the number of tokens per input, the training won‚Äôt even start. So be mindful of those settings when working with large models!\n\n\nV100 vs.¬†A100 vs.¬†Whatever GPU: What Difference Does It Make?\nSo, you might be wondering, ‚ÄúDoes it really matter which GPU I use?‚Äù Well, the short answer is: Absolutely! üèéÔ∏è\nIt‚Äôs like traveling from Canada to France. You could either fly or bike. Pretty obvious which one gets you there faster, right? The same logic applies to GPUs‚Äîmore powerful ones will get the job done quicker, especially when running large models.\nLet‚Äôs break it down:\n\nPowerful GPUs like the A100 not only handle large models better but also have extra features, like mixed-precision training, that can reduce memory usage and speed things up. They‚Äôre like the jet engines of the GPU world‚Äîfast, efficient, and capable of handling heavy workloads with ease.\nOlder or less powerful GPUs like the V100 can still do the job, but they might struggle with bigger models or more complex tasks. It‚Äôs like taking a slower flight‚Äîstill gets you there, but not as fast or smoothly.\nWhatever GPU you have, just remember: You need both memory and raw power to make your model run efficiently. If your GPU isn‚Äôt powerful enough, your model might take longer to run or could even crash. And nobody wants that!\n\nSo, whether you‚Äôre eyeing the latest A100 or making do with what you‚Äôve got, the takeaway is clear: A more powerful GPU means faster, more efficient model runs. If you need to go deeper, you can read this article too.\n\n\nAlternatives If Your Model Is Too Big\nGot a huge model with over a billion parameters? No worries, there are alternatives! You can try building a smaller model that uses fewer resources. Techniques like transfer learning, model quantization, or even using a smaller model altogether can help.\nHowever, keep in mind that these approaches might impact accuracy. Sometimes the trade-off isn‚Äôt that significant, and you can even set up safeguards for uncertain predictions, allowing the user to choose the most appropriate response. üí°\nFun fact: I‚Äôm working on a tool to anonymize text before feeding it to a large language model (LLM), and I use one of these techniques to run the model directly in the browser. Check it out here!\n\n\nTakeaways\nAlright, let‚Äôs wrap things up:\n\nEstimating exact memory requirements for your model can be tricky, but my tool can give you a good starting point.\nThe more powerful your GPU, the faster and smoother your model will run‚Äîso choose wisely!\nIf your model is too big, consider alternatives like downsizing or using techniques that reduce resource demands, though be mindful of potential accuracy trade-offs.\n\n\n\nGoodbye and Thanks!\nThanks for sticking with me through this post! I hope you found it helpful. If you have any thoughts, suggestions, or just want to say hi, feel free to reach out via email or drop an issue below. I‚Äôm always open to learning and would love to hear from you.\nTake care, and happy coding! üöÄ\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html",
    "href": "posts/predicting_comment_reply_llm.html",
    "title": "Predicting reply under comment with LLM",
    "section": "",
    "text": "This project originally started as a school assignment for Big Data class. The notebook presented here demonstrates the use of a large language model (LLM) to tackle a binary classification problem. Specifically, our objective is to predict whether a comment will receive a response or not.\nTo achieve this, we use an enriched dataset compiled from comments on Le Soleil‚Äôs Facebook posts. I will also share a separate notebook detailing the process of building this dataset. Additionally, I plan to publish another post explaining how to utilize simple feedforward neural networks or statistical models based on various comment features or the comment text itself.\nLet‚Äôs dive in!"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#context",
    "href": "posts/predicting_comment_reply_llm.html#context",
    "title": "Predicting reply under comment with LLM",
    "section": "",
    "text": "This project originally started as a school assignment for Big Data class. The notebook presented here demonstrates the use of a large language model (LLM) to tackle a binary classification problem. Specifically, our objective is to predict whether a comment will receive a response or not.\nTo achieve this, we use an enriched dataset compiled from comments on Le Soleil‚Äôs Facebook posts. I will also share a separate notebook detailing the process of building this dataset. Additionally, I plan to publish another post explaining how to utilize simple feedforward neural networks or statistical models based on various comment features or the comment text itself.\nLet‚Äôs dive in!"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#dependence",
    "href": "posts/predicting_comment_reply_llm.html#dependence",
    "title": "Predicting reply under comment with LLM",
    "section": "Dependence",
    "text": "Dependence\nI prefer to set aside the cell that install system dependency. It always produces a lot of useless gx3di3ce‚Ä¶ You get it, right ?\n\n\nCode\n!pip install torchsampler\n!pip install sacremoses\n\n\nNow, let‚Äôs import some packages to have fun !\n\n\nCode\nimport transformers, torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport multiprocessing as mp\nimport time\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom torchsampler import ImbalancedDatasetSampler\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom IPython.display import clear_output\n\n# warnings.filterwarnings('ignore')\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmp.cpu_count() # used to set the number of num_worker for Dataloader, usually the half of it \n\n\nAnd since I use Google colab, I mount my drive to load the datasets later.\n\n# only if you are using Google colab of course...\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#about-our-datasets",
    "href": "posts/predicting_comment_reply_llm.html#about-our-datasets",
    "title": "Predicting reply under comment with LLM",
    "section": "About our datasets",
    "text": "About our datasets\nIn this section, we will explore the dataset used for our binary classification problem. The dataset has been divided into training and testing sets, with 70% of the data allocated for training and the remaining 30% for testing. This split ensures that we have a robust training set to build our model while retaining a sufficient portion of the data for evaluating the model‚Äôs performance.\nLet‚Äôs load the train and the test sets.\n\ndirpath = '/content/drive/MyDrive/DataSets/big_data/datasets' # specify here the path to the dataset\ntrain = pd.read_csv(dirpath + '/split/train_dataset.csv', index_col=0)\ntest = pd.read_csv(dirpath + '/split/valid_dataset.csv', index_col=0)\n\nA few statistics about our dataset. First of all, it contains almost a million rows.\n\n\nCode\ndataset = pd.concat([train, test])\nprint(f'Dataset shape: {dataset.shape}')\n\n\nDataset shape: (935698, 68)\n\n\nSecondly, the dataset is highly unbalanced. The following graph shows that there are only ~13% of comments with replies, by which I mean that these comments have received at least one comment.\n\n\nCode\ndataset['target'].value_counts(normalize=True)\n\n\ntarget\nFalse    0.876036\nTrue     0.123964\nName: proportion, dtype: float64\n\n\n\n\nCode\ndataset['target'].value_counts().plot(kind='bar')\n\n\n\n\n\n\n\n\nFigure¬†1: Barchart of the count of each class\n\n\n\n\n\nWe have then two significant issues. Firstly, running the training on the entire dataset would be extremely time-consuming, even with substantial computational resources. Secondly, our dataset is unbalanced, which presents a challenge for accurate model training.\nTo address these issues, I opted for undersampling, ensuring an equal number of items from each class. This approach allows us to run our experiments more efficiently and mitigates the problem of data imbalance. Later in the notebook, we will explore the impact of the amount of data used on the model‚Äôs performance."
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#class-and-functions",
    "href": "posts/predicting_comment_reply_llm.html#class-and-functions",
    "title": "Predicting reply under comment with LLM",
    "section": "Class and functions",
    "text": "Class and functions\nI write the CommentDataset class as a custom dataset designed for handling text data. It inherits from the Dataset class provided by PyTorch. This class is specifically tailored for tokenizing and preparing text data along with their corresponding labels for use in a model.\n\n\nCode\nclass CommentDataset(Dataset):\n    def __init__(self, message, labels, tokenizer):\n        self.message = message\n        self.labels = labels\n        self.tokenizer = tokenizer\n\n    def get_labels(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.message)\n\n    def __getitem__(self, idx):\n        text = self.message[idx]\n        label = self.labels[idx]\n\n        inputs = self.tokenizer.encode_plus(text, None, add_special_tokens=True, padding='max_length', return_token_type_ids=True, truncation=True)\n\n        return {\n            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            'labels': torch.tensor(label, dtype=torch.float)\n        }\n\n\nThe train_model method is designed to train a machine learning model using a provided training and testing dataloader, while tracking various performance metrics such as loss, accuracy, precision, recall, and a custom F2 score across multiple epochs, and implementing early stopping based on validation performance. The test_model method evaluates the trained model on a validation dataset, computing and printing evaluation metrics to assess the model‚Äôs performance.\n\n\nCode\ndef train_model(model, train_dataloader, test_dataloader, history={}, num_epochs=5, lr=5e-5, early_stopping_patience=3, weight_decay=0.01):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=1)  # ReduceLROnPlateau scheduler\n    loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n\n    history['train_loss'] = []\n    history['train_accuracy'] = []\n    history['train_precision'] = []\n    history['train_recall'] = []\n    history['test_accuracy'] = []\n    history['test_precision'] = []\n    history['test_recall'] = []\n    history['epochs'] = []\n    history['test_loss'] = []\n    history['valid_score'] = []\n    best_valid_score = 0\n    early_stopping_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        train_preds = []\n        train_labels = []\n\n        # Training loop\n        for _, batch in enumerate(tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}')):\n            optimizer.zero_grad()\n\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            logits = outputs.logits.squeeze(1)\n            loss = loss_fn(logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_preds.extend((logits &gt; 0.5).int().tolist())\n            train_labels.extend(labels.tolist())\n\n        # Calculate metrics on training set\n        train_accuracy = accuracy_score(train_labels, train_preds)\n        train_precision = precision_score(train_labels, train_preds, average='binary')\n        train_recall = recall_score(train_labels, train_preds, average='binary')\n\n        # Evaluation loop\n        model.eval()\n        test_preds = []\n        test_labels = []\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for batch in test_dataloader:\n\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                token_type_ids = batch['token_type_ids'].to(device)\n                labels = batch['labels'].to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                logits = outputs.logits.squeeze(1)\n                loss = loss_fn(logits, labels)\n\n                test_loss += loss.item()\n                test_preds.extend((logits &gt; 0.5).int().tolist())\n                test_labels.extend(labels.tolist())\n\n        # Calculate metrics on test set\n        test_accuracy = accuracy_score(test_labels, test_preds)\n        test_precision = precision_score(test_labels, test_preds, average='binary')\n        test_recall = recall_score(test_labels, test_preds, average='binary')\n        tn, fp, fn, tp = confusion_matrix(test_labels, test_preds).ravel()\n        valid_score = (tp / (tp + fp + fn)) * 100\n\n        # Update learning rate scheduler\n        scheduler.step(valid_score)\n\n        history['epochs'].append(epoch + 1)\n\n        history['train_loss'].append(train_loss / len(train_dataloader))\n        history['train_accuracy'].append(train_accuracy)\n        history['train_precision'].append(train_precision)\n        history['train_recall'].append(train_recall)\n\n        history['test_loss'].append(test_loss / len(test_dataloader))\n        history['test_accuracy'].append(test_accuracy)\n        history['test_precision'].append(test_precision)\n        history['test_recall'].append(test_recall)\n        history['valid_score'].append(valid_score)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n\n        print(f\"  Train Loss: {train_loss / len(train_dataloader)}\")\n        print(f\"  Test Loss: {test_loss / len(test_dataloader)}\")\n        print(f\"  Train Accuracy: {train_accuracy}\")\n        print(f\"  Train Precision: {train_precision}\")\n        print(f\"  Train Recall: {train_recall}\")\n\n        print(f\"  Test Accuracy: {test_accuracy}\")\n        print(f\"  Test Precision: {test_precision}\")\n        print(f\"  Test Recall: {test_recall}\")\n        print(f\"  Test F2: {valid_score}\")\n\n        # Early stopping\n        if valid_score &gt; best_valid_score:\n            best_valid_score = valid_score\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n\n        if early_stopping_counter &gt;= early_stopping_patience:\n            print(\"Early stopping triggered!\")\n            break\n\ndef test_model(tokz, model, valid_data, history, device, bs = 16):\n    model.eval()\n    test_preds = []\n    test_labels = []\n    test_loss = 0.0\n\n    valid_dataset = CommentDataset(valid_data[0].to_numpy(), valid_data[1].astype(int).to_numpy(), tokz)\n    test_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=bs, shuffle=True)\n    loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            logits = outputs.logits.squeeze(1)\n            loss = loss_fn(logits, labels)\n\n            test_loss += loss.item()\n            test_preds.extend((logits &gt; 0.5).int().tolist())\n            test_labels.extend(labels.tolist())\n\n    test_accuracy = accuracy_score(test_labels, test_preds)\n    test_precision = precision_score(test_labels, test_preds)\n    test_recall = recall_score(test_labels, test_preds)\n    tn, fp, fn, tp = confusion_matrix(test_labels, test_preds).ravel()\n    history['valid_score'] = (tp / (tp + fp + fn)) * 100\n\n    print(\"Test Metrics:\")\n    print(f\"  Eval Accuracy: {test_accuracy}\")\n    print(f\"  Eval Precision: {test_precision}\")\n    print(f\"  Eval Recall: {test_recall}\")\n    print(f\"  Eval F2: {history['valid_score']}\")\n\n\nThe plot_history method visualizes the training and testing metrics (loss, accuracy, precision, recall, and F2 score) over epochs using matplotlib. The evaluate_model function assesses the model by optionally plotting the training history and running the test_model function for evaluation metrics. The get_loader function prepares the data loaders for training and testing datasets, including optional under-sampling, and sets up the tokenizer and model for sequence classification tasks. The equal_class_sampling method ensures balanced class distribution by sampling an equal number of instances from each class in the dataset.\n\n\nCode\ndef plot_history(history):\n    plt.figure(figsize=(17, 6))\n\n    epochs = history['epochs']\n    train_losses = history['train_loss']\n    test_loss = history['test_loss']\n    train_accuracies = history['train_accuracy']\n    test_accuracies = history['test_accuracy']\n    train_precisions = history['train_precision']\n    test_precisions = history['test_precision']\n    train_recall = history['train_recall']\n    test_recall = history['test_recall']\n    valid_score = history['valid_score']\n\n    plt.subplot(1, 5, 1)\n    plt.plot(epochs, train_losses, label='Training Loss')\n    plt.plot(epochs, test_loss, label='Test Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n\n    plt.subplot(1, 5, 2)\n    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.title('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 5, 3)\n    plt.plot(epochs, train_precisions, label='Training Precision')\n    plt.plot(epochs, test_precisions, label='Test Precision')\n    plt.xlabel('Epochs')\n    plt.ylabel('Precision')\n    plt.title('Precision')\n    plt.legend()\n\n    plt.subplot(1, 5, 4)\n    plt.plot(epochs, train_recall, label='Training Recall')\n    plt.plot(epochs, test_recall, label='Test Recall')\n    plt.xlabel('Epochs')\n    plt.ylabel('Recall')\n    plt.title('Recall')\n    plt.legend()\n\n    plt.subplot(1, 5, 5)\n    plt.plot(epochs, valid_score, label='Training F2')\n    plt.xlabel('Epochs')\n    plt.ylabel('F2')\n    plt.title('F2')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n    \ndef evaluate_model(tokz, model, valid_data, history, device, bs = 16, plot_train=True):\n    if plot_train:\n        plot_history(history)\n    test_model(tokz, model, valid_data, history, device, bs = bs)\n    \ndef get_loader(model_nm, dataset, bs = 100, under_sample=True, num_class = 1, use_pad_token=True, use_special_pad_token=False, num_workers=2):\n    X_train, y_train, X_test, y_test = dataset['X_train'], dataset['y_train'], dataset['X_test'], dataset['y_test']\n    tokz = AutoTokenizer.from_pretrained(model_nm)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels = num_class)\n\n    if len(X_train) == 0:\n      return model, tokz, None, None\n\n    if use_pad_token:\n        tokz.pad_token = tokz.eos_token\n    if use_special_pad_token:\n        tokz.add_special_tokens({'pad_token': '[PAD]'})\n\n    model.resize_token_embeddings(len(tokz))\n\n    train_dataset = CommentDataset(X_train.to_numpy(), y_train.astype(int).to_numpy(), tokz)\n    test_dataset = CommentDataset(X_test.to_numpy(), y_test.astype(int).to_numpy(), tokz)\n\n    if under_sample:\n        train_loader = torch.utils.data.DataLoader(train_dataset, sampler=ImbalancedDatasetSampler(train_dataset), batch_size=bs, num_workers=num_workers, pin_memory=True)\n        test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, batch_size=bs, num_workers=num_workers, pin_memory=True)\n    else:\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=True, num_workers=num_workers, pin_memory=True)\n\n    return model, tokz, train_loader, test_loader\n\ndef equal_class_sampling(input_features, target_labels, num_samples):\n    num_classes = len(target_labels.unique())\n    num_samples_per_class = num_samples // num_classes\n    dataset = pd.DataFrame({'input': input_features, 'target': target_labels})\n    grouped = dataset.groupby(['target'])\n    sampled_elements = grouped.apply(lambda x: x.sample(min(num_samples_per_class, len(x))))\n    return sampled_elements['input'], sampled_elements['target']"
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#training",
    "href": "posts/predicting_comment_reply_llm.html#training",
    "title": "Predicting reply under comment with LLM",
    "section": "Training",
    "text": "Training\n\nEvaluations\nTo evaluate our model, we will primarily use recall and a custom metric that we will call F2. Recall measures the ability of the model to correctly identify all relevant instances (true positives) from the dataset and is calculated as TP/(TP + FN), where TP stands for true positives and FN stands for false negatives.\nThe custom metric, F2, is designed to provide a more comprehensive evaluation of the model‚Äôs performance by balancing the detection of the positive class and minimizing errors. The F2 score is calculated as TP/(TP + FN + FP), where FP stands for false positives. This metric helps evaluate the model‚Äôs capacity to detect the positive class correctly while accounting for both false negatives and false positives. By considering both types of errors, the F2 metric ensures that the model is not only identifying positive instances accurately but also minimizing the incorrect classification of negative instances as positive. This balanced approach provides a more nuanced assessment of the model‚Äôs overall effectiveness.\n\n\n\nModeling\nFor simplicity‚Äôs sake, I‚Äôll use the distill version of Camembert model here, but you‚Äôre free to use any of the models below.\n\nmodels = {\n    'bert': \"bert-base-uncased\",\n    'gpt': \"distilgpt2\",\n    'flau': \"flaubert/flaubert_base_uncased\",\n    'cmb': \"cmarkea/distilcamembert-base\",\n}\n\n\n\nFine tuning\nThe next code might look a bit confusing at first, but let‚Äôs break it down step by step.\nWhat we‚Äôre doing here is creating smaller subsets from our original training and test sets to build training, validation and test samples.\n\n# Grab the 'message' and 'target' columns from the training set and store them in X_train and y_train\nX_train, y_train = train['message'], train['target']\n\nNext, we split off a small portion of the original test set to use as our validation set. This is like keeping a small piece of pie aside before sharing the rest.\n\nX_valid_sample, X_valid, y_valid_sample, y_valid = train_test_split(test['message'], test['target'], test_size=0.95, random_state=42)\nX_valid_sample.shape, X_valid.shape, y_valid_sample.shape, y_valid.shape\n\n((9357,), (177783,), (9357,), (177783,))\n\n\nThen, we balance our training data. Imagine we have 6000 rows, and we want to make sure we have an equal number of positive and negative samples‚Äî3000 of each.\n\nX_train_sample, y_train_sample = equal_class_sampling(X_train, y_train, 6000)\n\nFinally, we take another small slice of the test set to build our final test sample. Think of this as taking a tiny bit more of that pie for a taste test.\n\n# Split the remaining validation set to create a small test sample (2% of X_valid and y_valid)\n_, X_test_sample, _, y_test_sample = train_test_split(X_valid, y_valid, test_size=0.02, random_state=42)\nX_test_sample.shape\n\n(3556,)\n\n\nBy doing this, we ensure our model has balanced and representative data for training, validation, and final testing.\nNow, we train, we validate and evaluate the model on the test set.\n\n\nCode\nhistory = {}  # Initialize an empty dictionary to store training and evaluation history.\nBATCH_SIZE = 16  # Define the batch size for data loaders.\nLEARNING_RATE = 1e-4  # Set the learning rate for the optimizer.\nweight_decay = 1e-2  # Set the weight decay (L2 regularization) for the optimizer.\nEPOCHS = 10  # Set the number of epochs for training.\n\n# Prepare the dataset dictionary with training and testing samples.\ndata = {'X_train': X_train_sample, 'y_train': y_train_sample, 'X_test': X_test_sample, 'y_test': y_test_sample}\n\n# Get the model, tokenizer, training data loader, and testing data loader.\nmodel, tokz, train_loader, test_loader = get_loader(models['cmb'], data, bs=BATCH_SIZE, use_special_pad_token=True, num_workers=8)\n\nmodel.to(device)  # Move the model to the specified device (CPU or GPU).\n\nstart_time = time.time()  # Record the start time for training.\ntrain_model(model, train_loader, test_loader, history, num_epochs=EPOCHS, lr=LEARNING_RATE, early_stopping_patience=2, weight_decay=weight_decay)  # Train the model.\nend_time = time.time()  # Record the end time for training.\nexecution_time = end_time - start_time  # Calculate the execution time for training.\n\nclear_output()  # Clear the output (useful in Jupyter notebooks to clear previous outputs).\nprint(\"Execution time:\", execution_time, \"seconds\")  # Print the execution time for training.\n\nstart_time = time.time()  # Record the start time for evaluation.\n# Evaluate the model on the validation dataset and optionally plot the training history.\nevaluate_model(tokz, model, (X_valid_sample, y_valid_sample), history, device, bs = BATCH_SIZE * 2, plot_train=True)\nend_time = time.time()  # Record the end time for evaluation.\nexecution_time = end_time - start_time  # Calculate the execution time for evaluation.\nprint(\"Execution time:\", execution_time, \"seconds\")  # Print the execution time for evaluation.\n\n\nExecution time: 363.3319444656372 seconds\n\n\n\n\n\n\n\n\nFigure¬†2: Series of graphs depicting the performance metrics of the model. The metrics include Training Loss, Accuracy, Precision, Recall, and F2 score for both training and testing data.\n\n\n\n\n\nTest Metrics:\n  Eval Accuracy: 0.6475366036122688\n  Eval Precision: 0.23312101910828026\n  Eval Recall: 0.7605985037406484\n  Eval F2: 21.7184903868977\nExecution time: 84.7105667591095 seconds\n\n\nBased on the performance of the model on two epochs, we can make the following analysis:\n\nTraining Loss: Decreases from approximately 0.55 to 0.45, indicating improved performance during training.\nAccuracy: Shows a significant increase for training accuracy from about 0.70 to 0.78, while test accuracy slightly improves from around 0.62 to 0.64.\nPrecision: Training precision rises from about 0.75 to 0.80, whereas test precision remains almost constant at around 0.21.\nRecall: Training recall increases from 0.60 to 0.75, while test recall decreases from 0.85 to 0.70, suggesting potential overfitting.\nF2: Indicates a decline in the F2 score from approximately 21.2 to 20.2, which means that the model is not generalizing from the training data to the test data.\n\n\nTest Metrics:\n\nEval Accuracy: 0.6475, which indicates the model‚Äôs ability to correctly predict test data is moderate but lower compared to training accuracy.\nEval Precision: 0.2331, which is significantly lower than the training precision, suggesting the model struggles with false positives on the test set.\nEval Recall: 0.7606, which is relatively high and close to the training recall, showing the model still performs well in identifying most positive instances on the test set.\nEval F2: 21.7185, which remains high, indicating that despite high recall, the model struggles with false positives.\n\n\n\nConclusion:\n\nOverfitting: The discrepancy between training and test precision suggests overfitting. The model performs well on the training data but struggles with generalization, leading to lower performance on unseen data.\nF2 score: The model prioritizes recall over precision. This is evident from the high recall but low precision on the test set. This behavior is further reflected in the F2 score, which is low, indicating many false positives.\n\nThis analysis suggest that there is a problem with our model, because we need a model that should perform well on unseen data with low errors.\n\n\nWhy is this important?\nWell, imagine that we will deploy our model in a real-world application. We don‚Äôt want to miss comments that might receive a response because we could use them to increase traffic on our site or social media. In that case, a model that detects positive instances well with minimal false positives is acceptable. However, our model currently has many false positives, which can be problematic.\n\n\nPractical Implications:\n\nBusiness Impact: If the model is used in an application like content moderation or customer feedback analysis, high false positives mean that many irrelevant comments would be flagged for response. This can lead to inefficient use of resources and missed opportunities to engage with truly relevant comments.\nUser Experience: In applications like spam detection, a high number of false positives can frustrate users, as legitimate messages may be incorrectly flagged as spam.\nOperational Efficiency: For customer service applications, responding to false positives wastes time and effort that could be better spent addressing genuine issues.\n\nBut, let‚Äôs try with more data in our training set to see the impact on the model performance. We will initialise a new model and train it on 10000 comments.\n\n\nCode\nX_train, y_train = train['message'], train['target']\nX_train_sample, y_train_sample = equal_class_sampling(X_train, y_train, 10000)\n_, X_test_sample, _, y_test_sample = train_test_split(X_valid, y_valid, test_size=0.02, random_state=42)\n\n\n\n\nCode\nhistory = {}\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-4\nweight_decay = 1e-2\nEPOCHS = 10\ndata = {'X_train': X_train_sample, 'y_train': y_train_sample, 'X_test': X_test_sample, 'y_test': y_test_sample}\nmodel, tokz, train_loader, test_loader = get_loader(models['cmb'], data, bs=BATCH_SIZE, use_special_pad_token=True, num_workers=8)\nmodel.to(device)\n\nstart_time = time.time()\ntrain_model(model, train_loader, test_loader, history, num_epochs=EPOCHS, lr=LEARNING_RATE, early_stopping_patience=2, weight_decay=weight_decay)\nend_time = time.time()\nexecution_time = end_time - start_time\n\nclear_output()\nprint(\"Execution time:\", execution_time, \"seconds\")\n\nstart_time = time.time()\nevaluate_model(tokz, model, (X_valid_sample, y_valid_sample), history, device, bs = BATCH_SIZE * 2, plot_train=True)\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(\"Execution time:\", execution_time, \"seconds\")\n\n\nExecution time: 1122.6102643013 seconds\n\n\n\n\n\n\n\n\nFigure¬†3: Series of graphs depicting the performance metrics of the model. The metrics include Training Loss, Accuracy, Precision, Recall, and F2 score for both training and testing data.\n\n\n\n\n\nTest Metrics:\n  Eval Accuracy: 0.6856898578604254\n  Eval Precision: 0.25793871866295265\n  Eval Recall: 0.769742310889443\n  Eval F2: 23.946211533488494\nExecution time: 84.70709776878357 seconds\n\n\nThe F2 score improved from 21% to 23%. But can we conclude that it‚Äôs the increased training set that induces these results?\nLet‚Äôs try it with a larger training set.\n\n\nCode\nX_train, y_train = train['message'], train['target']\nX_train_sample, y_train_sample = equal_class_sampling(X_train, y_train, 15000)\n_, X_test_sample, _, y_test_sample = train_test_split(X_valid, y_valid, test_size=0.02, random_state=42)\n\n\n\n\nCode\nhistory = {}\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-4\nweight_decay = 1e-2\nEPOCHS = 10\ndata = {'X_train': X_train_sample, 'y_train': y_train_sample, 'X_test': X_test_sample, 'y_test': y_test_sample}\nmodel, tokz, train_loader, test_loader = get_loader(models['cmb'], data, bs=BATCH_SIZE, use_special_pad_token=True, num_workers=8)\nmodel.to(device)\n\nstart_time = time.time()\ntrain_model(model, train_loader, test_loader, history, num_epochs=EPOCHS, lr=LEARNING_RATE, early_stopping_patience=2, weight_decay=weight_decay)\nend_time = time.time()\nexecution_time = end_time - start_time\n\nclear_output()\nprint(\"Execution time:\", execution_time, \"seconds\")\n\nstart_time = time.time()\nevaluate_model(tokz, model, (X_valid_sample, y_valid_sample), history, device, bs = BATCH_SIZE * 2, plot_train=True)\nend_time = time.time()\nexecution_time = end_time - start_time\nprint(\"Execution time:\", execution_time, \"seconds\")\n\n\nExecution time: 1914.5363965034485 seconds\n\n\n\n\n\n\n\n\nFigure¬†4: Series of graphs depicting the performance metrics of the model. The metrics include Training Loss, Accuracy, Precision, Recall, and F2 score for both training and testing data.\n\n\n\n\n\nTest Metrics:\n  Eval Accuracy: 0.7079192048733568\n  Eval Precision: 0.26705237515225333\n  Eval Recall: 0.7290108063175395\n  Eval F2: 24.293628808864266\nExecution time: 85.33442664146423 seconds\n\n\nOn test set, the F2 score of the model improve again from 23% to 24%.\n\n\nConclusion:\nThe increase in the training set size helps the model generalize better to the test data, reducing overfitting and improving its ability to balance precision and recall effectively."
  },
  {
    "objectID": "posts/predicting_comment_reply_llm.html#takeaways",
    "href": "posts/predicting_comment_reply_llm.html#takeaways",
    "title": "Predicting reply under comment with LLM",
    "section": "Takeaways",
    "text": "Takeaways\n\nDataset Analysis is Crucial: Always begin by analyzing your dataset. Understanding the distribution and characteristics of your data helps in making informed decisions about model training and evaluation. In scenarios with limited resources, generating more data for the underrepresented class might not be feasible. Instead, sampling equal numbers of comments from each class for the training set can help reduce bias towards the overrepresented class.\nBalanced Training, Unbalanced Validation: While balancing the training set by equal sampling is important to reduce bias, the validation set should remain unbalanced. This approach ensures that the model‚Äôs performance is evaluated in a realistic manner, reflecting its ability to generalize to the true distribution of the data.\nResource-Based Training Strategy: Define your training strategy based on the available resources. When computational power or time is limited, working with a smaller, balanced sample of the dataset is a practical approach. This allows for iterative experimentation and tuning without the overhead of processing the entire dataset.\nProblem-Specific Metrics: Choose evaluation metrics that align with your problem‚Äôs objectives. For instance, in this scenario, the F2 score (F2 = tp / (tp + 2 * fn + fp)) is used to evaluate model performance by balancing the detection of the positive class and minimizing errors.\nInitial Model Performance: After the first round of training, the F2 score indicates that the model prioritizes recall over precision. This is evidenced by the high recall but low precision on the test set.\nImpact of False Positives: High false positives can be problematic in real-world applications. They can lead to inefficient use of resources and missed opportunities to engage with truly relevant comments. This highlights the need for a balance between precision and recall.\nTraining Set Size and Generalization: Increasing the size of the training set helps the model generalize better to the test data. A larger training set reduces overfitting and enhances the model‚Äôs ability to balance precision and recall effectively. This results in improved overall performance and more reliable predictions.\nChoosing the Right Model: Select a model that is suitable for your specific problem. For instance, since the dataset consists of French text, using CamemBERT, a model specifically designed for the French language, is an appropriate choice.\nHyperparameter Tuning: Finding the optimal hyperparameters for your model is crucial and often involves extensive experimentation. Before finalizing the model, numerous combinations were tested to identify the best-performing configuration. Hyperparameter tuning is more of an art than a strict recipe, requiring intuition and experience to achieve the best results."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "multiverse",
    "section": "",
    "text": "How Much Memory Does Your Model Need on GPU? Let‚Äôs Find Out!\n\n\n\n\n\n\nTips & Tricks\n\n\nPersonal Projects\n\n\n\nAn exploration of GPU memory requirements for running models and how to estimate them with a handy tool.\n\n\n\n\n\n24 ao√ªt 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, An Introduction: The Story Behind This Blog and What‚Äôs Next\n\n\n\n\n\n\nPersonal Projects\n\n\nUpdates\n\n\n\nIn this post, I finally take the time to properly introduce my blog, explain its purpose, and share why I‚Äôve been away for the past few months. It‚Äôs a fresh start and an exciting journey ahead!\n\n\n\n\n\n23 ao√ªt 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)\n\n\n\n\n\n\nTips & Tricks\n\n\nPersonal Projects\n\n\n\nDiscover a handy tool that anonymizes your text before using it in LLMs, keeping your personal information safe and sound!\n\n\n\n\n\n23 ao√ªt 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPasser de Yolo √† CocoDetection, c‚Äôest possible !\n\n\n\n\n\n\nTips & Tricks\n\n\nTutorial\n\n\n\nJe partage dans cet article une m√©thode simple pour convertir un dataset YOLO au format COCO, ou √† un autre, une solution que j‚Äôai trouv√©e essentielle lors d‚Äôun r√©cent projet de segmentation d‚Äôinstance.\n\n\n\n\n\n30 mai 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting reply under comment with LLM\n\n\n\n\n\n\nTutorial\n\n\n\nFinetuning of Camembert on predicting, using a Le Soleil Facebook post comment as input, whether it will receive a response or not.\n\n\n\n\n\n21 mai 2024\n\n\n\n\n\n\nAucun article correspondant\n\n Retour au sommet",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "works.html",
    "href": "works.html",
    "title": "Personal works and other stuffs",
    "section": "",
    "text": "I want to share what I‚Äôve accomplished as projects, in AI, in software engineering ‚Äì and maybe some other areas too. üöÄ This is not an exhaustive list, but it‚Äôs a start! I‚Äôll be updating this page regularly with new projects and articles, so keep an eye out for more. üåü I‚Äôm also working on a few projects I won‚Äôt be able to share here but I will publish some articles explaining them as soon as possible. Stay tuned! üöß",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#in-progress",
    "href": "works.html#in-progress",
    "title": "Personal works and other stuffs",
    "section": "In Progress",
    "text": "In Progress\nHere are some of the projects I‚Äôm currently working on:\ncoming soon‚Ä¶",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#in-ai",
    "href": "works.html#in-ai",
    "title": "Personal works and other stuffs",
    "section": "In AI",
    "text": "In AI\nIt‚Äôs been a while since I started my journey into AI. Here are some of the projects I‚Äôve worked on:\ncoming soon‚Ä¶",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#in-software-engineering",
    "href": "works.html#in-software-engineering",
    "title": "Personal works and other stuffs",
    "section": "In software engineering",
    "text": "In software engineering\nBefore starting my master‚Äôs degree in Artificial Intelligence, I worked on a few projects in software engineering. Here are some of them:\ncoming soon‚Ä¶",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#found-this-interesting",
    "href": "works.html#found-this-interesting",
    "title": "Personal works and other stuffs",
    "section": "Found this interesting?",
    "text": "Found this interesting?\nLet me know if you have any comments or ideas on how I can improve. I‚Äôm all ears! üëÇ I‚Äôll be updating regularly on my progress and any news, so keep an eye out for more. Thank you for reading! üíñ",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "",
    "text": "AI Developer fluent in machine learning and web technologies, I‚Äôm passionate about leveraging AI to tackle real-world problems and create a tangible impact. My primary interests lie in revolutionizing industries such as food production and finance through innovative technological solutions. With a deep commitment to sharing knowledge and fostering collaboration, I actively contribute to open-source projects and engage with the tech community.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#experiences",
    "href": "about.html#experiences",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Experiences",
    "text": "Experiences\n\nLead Developer | Sept 2022 - Jul 2023\nAgroSfer | Cotonou, B√©nin\nLead development teams in creating a dynamic data aggregation system for enhanced dashboard analytics. Utilized Angular, Laravel, and MongoDB technologies.\n\n\nFront-end Developer | Jun 2021 - Sept 2022\nAgroSfer | Cotonou, B√©nin\nStudied advanced software systems, focusing on practical and theoretical aspects of software engineering in Benin.\n\n\nInternship in Web Development | Mars 2021 - Mai 2021\nLa Vedette Media Digital | Cotonou, B√©nin\nDesign and development of the architecture for an intranet-based project management web platform for La Vedette Media.\n\n\nInternship in Web Development | June 2020 - Sept 2020\nNAUTILUS TECHNOLOGY | Cotonou, B√©nin\nInternship role focusing on web development using JavaScript, Angular, Laravel, and Git, solidifying foundational web development skills.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Education",
    "text": "Education\n\nMa√Ætrise en informatique - Intelligence Artificielle | Since Sept 2023\nUniversit√© Laval | Qu√©bec, CA\nPursuing a Master‚Äôs degree focusing on Artificial Intelligence, enhancing skills in complex AI systems and machine learning implementations.\n\n\nSyst√®mes informatiques et Logiciels | Oct 2018 - Jul 2021\nLes COURS SONOU | Abomey-Calavi, B√©nin\nStudied advanced software systems, focusing on practical and theoretical aspects of software engineering in Benin.\n\n\nBaccalaur√©at Science Maths | Ao√ªt 2018\nCEG Application | Porto-Novo, B√©nin\nGraduated with a focus on Mathematics and Science, forming a strong analytical foundation.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Skills",
    "text": "Skills\n\nArtificial Intelligence\nPython, Pytorch, Tensorflow, Keras\nMatplotlib, Seaborn, Numpy, Pandas, timm, fastai, fastcore, torchvision, Poutyne,\nSPSS Modeler, RStudio\n\n\nSoftware engineering\nPHP, Javascript, C, C++, Java\nAngular, ReactJs, VueJs, Cypress, Web API\nSQL, Mysql, Firebase, MongoDB, Web Services, APIs Rest\n\n\nOthers\nGit, DevOps, CI/CD, UX Design, Scrum, Agile",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#online-courses",
    "href": "about.html#online-courses",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Online courses",
    "text": "Online courses\n\nFrom Engineer to Technical Manager: A Survival Guide, Sundog Education\nUdemy - 2023",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "posts/for_you.html",
    "href": "posts/for_you.html",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "",
    "text": "Alright, let‚Äôs be real. Life happened, right? üòÖ I took some time to wrap up a few things in my life and, honestly, to kickstart a new chapter. You know that feeling when you finish one thing and feel like a whole new world opens up? Yeah, that‚Äôs where I‚Äôm at! üåü\nPlus, it was the end of the school session, and I needed some well-deserved downtime. Relaxing, de-stressing, and just enjoying life for a bit. But guess what? Now I‚Äôm back, energized, and ready to take on the world! üåç‚ú®\nI‚Äôve realized how much I‚Äôve already accomplished, and let me tell you, I‚Äôm super grateful for all of it. üôè But I also see how much more there is to do. My blog, my job, my personal life ‚Äì they all need my attention, and I‚Äôm ready to dive in.\nThe possibilities ahead? Endless. I‚Äôm seizing every opportunity coming my way, and I can‚Äôt wait to share the journey with you! üöÄ"
  },
  {
    "objectID": "posts/for_you.html#what-happened-indeed",
    "href": "posts/for_you.html#what-happened-indeed",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "",
    "text": "Alright, let‚Äôs be real. Life happened, right? üòÖ I took some time to wrap up a few things in my life and, honestly, to kickstart a new chapter. You know that feeling when you finish one thing and feel like a whole new world opens up? Yeah, that‚Äôs where I‚Äôm at! üåü\nPlus, it was the end of the school session, and I needed some well-deserved downtime. Relaxing, de-stressing, and just enjoying life for a bit. But guess what? Now I‚Äôm back, energized, and ready to take on the world! üåç‚ú®\nI‚Äôve realized how much I‚Äôve already accomplished, and let me tell you, I‚Äôm super grateful for all of it. üôè But I also see how much more there is to do. My blog, my job, my personal life ‚Äì they all need my attention, and I‚Äôm ready to dive in.\nThe possibilities ahead? Endless. I‚Äôm seizing every opportunity coming my way, and I can‚Äôt wait to share the journey with you! üöÄ"
  },
  {
    "objectID": "posts/for_you.html#whats-next",
    "href": "posts/for_you.html#whats-next",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "What‚Äôs Next? üéâ",
    "text": "What‚Äôs Next? üéâ\nFirst off, a huge shoutout to my brothers and my family. Without them, I wouldn‚Äôt be here doing what I love. You guys are the real MVPs! üí™\nNow, onto me. I‚Äôm committed to working on myself, becoming the best version of me. I‚Äôm just getting started on my journey, my career, my dreams ‚Äì and I‚Äôm 100% ready to make them a reality. Let‚Äôs do this! üôå"
  },
  {
    "objectID": "posts/for_you.html#the-reason-behind-this-blog",
    "href": "posts/for_you.html#the-reason-behind-this-blog",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "The Reason Behind This Blog üíª",
    "text": "The Reason Behind This Blog üíª\nSo, why this blog? Well, it‚Äôs simple. I want to share what I‚Äôve accomplished in my career, in AI, in software engineering ‚Äì and maybe some other areas too. üöÄ\nBut here‚Äôs the thing: this isn‚Äôt going to be a blog where I show you how to do things or preach about the future of AI (though, who knows, maybe a little bit of that üòâ). Instead, it‚Äôs going to be a place where I share what I‚Äôm learning, what I‚Äôm building, and who I‚Äôm building it with. And hey, I‚Äôd love to hear your thoughts, comments, and recommendations along the way. Don‚Äôt hesitate! üòä"
  },
  {
    "objectID": "posts/for_you.html#why-now",
    "href": "posts/for_you.html#why-now",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "Why Now? ü§∑",
    "text": "Why Now? ü§∑\nYou know, I just realized that I never really took the time to properly introduce this blog or explain what it‚Äôs all about. Better late than never, right? üòÅ So, here it is! üéâ"
  },
  {
    "objectID": "posts/for_you.html#how-its-built",
    "href": "posts/for_you.html#how-its-built",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "How It‚Äôs Built üõ†Ô∏è",
    "text": "How It‚Äôs Built üõ†Ô∏è\nFor all the techies out there, I‚Äôm using Quarto, and it‚Äôs deployed with GitHub Pages. I wanted to keep things simple but also have the flexibility to share code. Quarto‚Äôs been perfect for that ‚Äì super interesting and easy to get the hang of. üëç"
  },
  {
    "objectID": "posts/for_you.html#what-else",
    "href": "posts/for_you.html#what-else",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "What Else? üßê",
    "text": "What Else? üßê\nOh, and one more thing ‚Äì I‚Äôve added a page to showcase all the projects I‚Äôm working on, both finished and in progress. I‚Äôll be publishing articles explaining them, sharing why I find them interesting, and how they‚Äôre coming along. Stay tuned! üöß\nGot any comments or ideas on how I can improve? I‚Äôm all ears! üëÇ I‚Äôll be updating regularly on my progress and any news, so keep an eye out for more.\nAnd hey, I speak French too! Je peux bien parler en fran√ßais qu‚Äôen anglais, permettez-moi ce petit √©cart üòâ."
  },
  {
    "objectID": "posts/for_you.html#ok-time-to-say-goodbye",
    "href": "posts/for_you.html#ok-time-to-say-goodbye",
    "title": "Finally, An Introduction: The Story Behind This Blog and What‚Äôs Next",
    "section": "Ok, Time to Say Goodbye üëã",
    "text": "Ok, Time to Say Goodbye üëã\nThat‚Äôs all for now, folks! See you soon, and take care! Love you all! üíñ"
  },
  {
    "objectID": "posts/anonymizer_intro.html",
    "href": "posts/anonymizer_intro.html",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "",
    "text": "Hey folks! üôå\nEver find yourself using those cool new LLMs like ChatGPT for pretty much everything‚Äîwhether it‚Äôs writing emails, drafting cover letters, or even creating blog content (I know I do! üòÖ)? It‚Äôs like having your personal assistant right there in your browser, making life so much easier. But, wait‚Ä¶what happens when your prompts contain personal details? üò¨\nYou know, that awkward moment when you realize you‚Äôve shared too much in a prompt? Yeah, been there, done that. So, I figured, why not build a tool that handles this issue and keeps things private? üí°"
  },
  {
    "objectID": "posts/anonymizer_intro.html#how-i-found-myself-working-on-this",
    "href": "posts/anonymizer_intro.html#how-i-found-myself-working-on-this",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "How I Found Myself Working on This üõ†Ô∏è",
    "text": "How I Found Myself Working on This üõ†Ô∏è\nRecently, I noticed just how much I‚Äôve been relying on LLMs for personal stuff‚Äîwriting emails, tweaking my CV, crafting blogs, even coding! But every time I use them, I have to manually remove or anonymize personal info in my prompts. Talk about tedious! üò©\nIt got to a point where I was doing this so often that I thought, ‚ÄúThere‚Äôs got to be a better way!‚Äù And so, the idea of building a tool that runs entirely offline, right in your browser, was born. üöÄ"
  },
  {
    "objectID": "posts/anonymizer_intro.html#the-idea",
    "href": "posts/anonymizer_intro.html#the-idea",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "The Idea üí°",
    "text": "The Idea üí°\nIt‚Äôs super simple! Just paste your text, and voila‚Äîit anonymizes it. Now, you can safely use it in your prompts without worrying about personal details leaking out. ‚ú®\n\nExample Time! ‚úçÔ∏è\nImagine you‚Äôve have to send an email to a customer about an upcoming event at your coffee shop. Here‚Äôs what it might look like:\n\nSubject: Join us for the Grand Renovation of New York - Exciting Changes Await!\nDear John Green,\nWe hope this email finds you well and enjoying your favorite vanilla latte large and banana muffin at our coffee shop! As a valued and loyal customer, we are thrilled to share some exciting news with you.\nMark your calendar for January 4, 2023, as we invite you to join us for the grand renovation of our coffee shop at New York! We are embarking on a journey to create an even better and cozier space for you to relax, indulge, and enjoy your favorite beverages and treats.\nYou have been a cherished part of our coffee shop family for an incredible 5 years, and we cannot wait to show our appreciation by unveiling a fresh, modern look that will elevate your coffee shop experience. The renovation will bring about stylish and comfortable seating areas, captivating artwork, and state-of-the-art equipment to enhance the quality of our offerings.\nThe grand reopening event promises to be a celebration filled with delightful surprises. As our special guest, you will be among the first to explore the redesigned space and sample our latest menu additions. Immerse yourself in the inviting ambiance, meet our talented baristas, and indulge in the aroma of freshly brewed coffee that is synonymous with our coffee shop. We value your opinion, John Green, and would be thrilled to hear your thoughts and feedback on our newly renovated space. As always, our commitment remains to provide you with exceptional customer service and the highest quality products that you know and love.\nSave the date: January 4, 2023! We cannot wait to welcome you to the grand renovation of our coffee shop at New York. Together, let‚Äôs embark on this exciting journey and create countless new memories over aromatic coffees and delectable treats.\nThank you for being an integral part of our coffee shop‚Äôs success. We look forward to continuing to serve you and make your coffee experiences unforgettable.\nWarmest regards,\n\nYou paste it into the tool, and poof‚Äîit replaces those details with anonymous IDs. üïµÔ∏è‚Äç‚ôÄÔ∏è\n\nSubject: Join us for the Grand Renovation of aLOCATION_524a - Exciting Changes Await!\nDear aPEOPLE_872a,\nWe hope this email finds you well and enjoying your favorite vanilla latte large and banana muffin at our coffee shop! As a valued and loyal customer, we are thrilled to share some exciting news with you.\nMark your calendar for aDATE_988a, as we invite you to join us for the grand renovation of our coffee shop at aLOCATION_524a! We are embarking on a journey to create an even better and cozier space for you to relax, indulge, and enjoy your favorite beverages and treats.\nYou have been a cherished part of our coffee shop family for an incredible 5 years, and we cannot wait to show our appreciation by unveiling a fresh, modern look that will elevate your coffee shop experience. The renovation will bring about stylish and comfortable seating areas, captivating artwork, and state-of-the-art equipment to enhance the quality of our offerings.\nThe grand reopening event promises to be a celebration filled with delightful surprises. As our special guest, you will be among the first to explore the redesigned space and sample our latest menu additions. Immerse yourself in the inviting ambiance, meet our talented baristas, and indulge in the aroma of freshly brewed coffee that is synonymous with our coffee shop. We value your opinion, aPEOPLE_872a, and would be thrilled to hear your thoughts and feedback on our newly renovated space. As always, our commitment remains to provide you with exceptional customer service and the highest quality products that you know and love.\nSave the date: aDATE_988a! We cannot wait to welcome you to the grand renovation of our coffee shop at aLOCATION_524a. Together, let‚Äôs embark on this exciting journey and create countless new memories over aromatic coffees and delectable treats.\nThank you for being an integral part of our coffee shop‚Äôs success. We look forward to continuing to serve you and make your coffee experiences unforgettable.\nWarmest regards,\n\nThen, you can use that anonymized text in your LLM prompt. Once you get the LLM‚Äôs response, you paste it back into the tool, and it replaces the IDs with your original info. As simple as that! üòé"
  },
  {
    "objectID": "posts/anonymizer_intro.html#how-to-try-it",
    "href": "posts/anonymizer_intro.html#how-to-try-it",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "How to Try It? ü§î",
    "text": "How to Try It? ü§î\nExcited to give it a spin? You can check it out here. It‚Äôs still a work in progress (so don‚Äôt judge me too harshly! üòú), but the full offline version is coming soon.\nFor now, you can test it out and share your thoughts with me. Your feedback means the world! üåç"
  },
  {
    "objectID": "posts/anonymizer_intro.html#next-steps",
    "href": "posts/anonymizer_intro.html#next-steps",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "Next Steps üöÄ",
    "text": "Next Steps üöÄ\nHere‚Äôs what I‚Äôve got cooking for the future:\n\nTurning it into a browser extension that automatically catches text you paste into input boxes, helping protect your identity on the fly.\nExtending the tool to handle PII (Personally Identifiable Information) with customizable options for what to anonymize.\nAnd so much more! üéâ"
  },
  {
    "objectID": "posts/anonymizer_intro.html#but-why",
    "href": "posts/anonymizer_intro.html#but-why",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "But Why? ü§∑‚Äç‚ôÇÔ∏è",
    "text": "But Why? ü§∑‚Äç‚ôÇÔ∏è\nGood question! There are already some awesome tools out there, like this one and this GitHub repo. But none of them are quite what I was looking for‚Äîso I decided to share mine! üòÑ\nIf you‚Äôve got any comments, feel free to reach out or open an issue. I‚Äôm all ears! üëÇ"
  },
  {
    "objectID": "posts/anonymizer_intro.html#it-was-fun-and-educational",
    "href": "posts/anonymizer_intro.html#it-was-fun-and-educational",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "It Was Fun and Educational! üéì",
    "text": "It Was Fun and Educational! üéì\nBuilding this tool was such a learning experience. I dove into ONNX, WebGPU, WebAssembly, Transformer.js, Hugging Face Inference, Angular 18, and so much more. And, boy, did I learn a lot! üíª\n\nAlso, a Few Things I Noticed‚Ä¶ üßê\n\nRunning an LLM in the browser without a GPU can be super slow. üê¢\nNot all browsers support WebGPU, so that‚Äôs a challenge.\nTo run a model in the browser, you need a really light model, preferably quantized and optimized for the web (like with Olive, for example). Ideally, it should run on WebGPU for the best performance."
  },
  {
    "objectID": "posts/anonymizer_intro.html#final-thoughts",
    "href": "posts/anonymizer_intro.html#final-thoughts",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "Final Thoughts‚Ä¶ üéâ",
    "text": "Final Thoughts‚Ä¶ üéâ\nOverall, it‚Äôs been a fantastic experience. Version 2 is already in the works, so stay tuned! üòé\nThanks so much for reading, and don‚Äôt hesitate to reach out with your thoughts.\nCheers! ü•Ç"
  }
]