[
  {
    "objectID": "posts/for_you.html",
    "href": "posts/for_you.html",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "",
    "text": "Alright, letâ€™s be real. Life happened, right? ğŸ˜… I took some time to wrap up a few things in my life and, honestly, to kickstart a new chapter. You know that feeling when you finish one thing and feel like a whole new world opens up? Yeah, thatâ€™s where Iâ€™m at! ğŸŒŸ\nPlus, it was the end of the school session, and I needed some well-deserved downtime. Relaxing, de-stressing, and just enjoying life for a bit. But guess what? Now Iâ€™m back, energized, and ready to take on the world! ğŸŒâœ¨\nIâ€™ve realized how much Iâ€™ve already accomplished, and let me tell you, Iâ€™m super grateful for all of it. ğŸ™ But I also see how much more there is to do. My blog, my job, my personal life â€“ they all need my attention, and Iâ€™m ready to dive in.\nThe possibilities ahead? Endless. Iâ€™m seizing every opportunity coming my way, and I canâ€™t wait to share the journey with you! ğŸš€"
  },
  {
    "objectID": "posts/for_you.html#what-happened-indeed",
    "href": "posts/for_you.html#what-happened-indeed",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "",
    "text": "Alright, letâ€™s be real. Life happened, right? ğŸ˜… I took some time to wrap up a few things in my life and, honestly, to kickstart a new chapter. You know that feeling when you finish one thing and feel like a whole new world opens up? Yeah, thatâ€™s where Iâ€™m at! ğŸŒŸ\nPlus, it was the end of the school session, and I needed some well-deserved downtime. Relaxing, de-stressing, and just enjoying life for a bit. But guess what? Now Iâ€™m back, energized, and ready to take on the world! ğŸŒâœ¨\nIâ€™ve realized how much Iâ€™ve already accomplished, and let me tell you, Iâ€™m super grateful for all of it. ğŸ™ But I also see how much more there is to do. My blog, my job, my personal life â€“ they all need my attention, and Iâ€™m ready to dive in.\nThe possibilities ahead? Endless. Iâ€™m seizing every opportunity coming my way, and I canâ€™t wait to share the journey with you! ğŸš€"
  },
  {
    "objectID": "posts/for_you.html#whats-next",
    "href": "posts/for_you.html#whats-next",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "Whatâ€™s Next? ğŸ‰",
    "text": "Whatâ€™s Next? ğŸ‰\nFirst off, a huge shoutout to my brothers and my family. Without them, I wouldnâ€™t be here doing what I love. You guys are the real MVPs! ğŸ’ª\nNow, onto me. Iâ€™m committed to working on myself, becoming the best version of me. Iâ€™m just getting started on my journey, my career, my dreams â€“ and Iâ€™m 100% ready to make them a reality. Letâ€™s do this! ğŸ™Œ"
  },
  {
    "objectID": "posts/for_you.html#the-reason-behind-this-blog",
    "href": "posts/for_you.html#the-reason-behind-this-blog",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "The Reason Behind This Blog ğŸ’»",
    "text": "The Reason Behind This Blog ğŸ’»\nSo, why this blog? Well, itâ€™s simple. I want to share what Iâ€™ve accomplished in my career, in AI, in software engineering â€“ and maybe some other areas too. ğŸš€\nBut hereâ€™s the thing: this isnâ€™t going to be a blog where I show you how to do things or preach about the future of AI (though, who knows, maybe a little bit of that ğŸ˜‰). Instead, itâ€™s going to be a place where I share what Iâ€™m learning, what Iâ€™m building, and who Iâ€™m building it with. And hey, Iâ€™d love to hear your thoughts, comments, and recommendations along the way. Donâ€™t hesitate! ğŸ˜Š"
  },
  {
    "objectID": "posts/for_you.html#why-now",
    "href": "posts/for_you.html#why-now",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "Why Now? ğŸ¤·",
    "text": "Why Now? ğŸ¤·\nYou know, I just realized that I never really took the time to properly introduce this blog or explain what itâ€™s all about. Better late than never, right? ğŸ˜ So, here it is! ğŸ‰"
  },
  {
    "objectID": "posts/for_you.html#how-its-built",
    "href": "posts/for_you.html#how-its-built",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "How Itâ€™s Built ğŸ› ï¸",
    "text": "How Itâ€™s Built ğŸ› ï¸\nFor all the techies out there, Iâ€™m using Quarto, and itâ€™s deployed with GitHub Pages. I wanted to keep things simple but also have the flexibility to share code. Quartoâ€™s been perfect for that â€“ super interesting and easy to get the hang of. ğŸ‘"
  },
  {
    "objectID": "posts/for_you.html#what-else",
    "href": "posts/for_you.html#what-else",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "What Else? ğŸ§",
    "text": "What Else? ğŸ§\nOh, and one more thing â€“ Iâ€™ve added a page to showcase all the projects Iâ€™m working on, both finished and in progress. Iâ€™ll be publishing articles explaining them, sharing why I find them interesting, and how theyâ€™re coming along. Stay tuned! ğŸš§\nGot any comments or ideas on how I can improve? Iâ€™m all ears! ğŸ‘‚ Iâ€™ll be updating regularly on my progress and any news, so keep an eye out for more.\nAnd hey, I speak French too! Je peux bien parler en franÃ§ais quâ€™en anglais, permettez-moi ce petit Ã©cart ğŸ˜‰."
  },
  {
    "objectID": "posts/for_you.html#ok-time-to-say-goodbye",
    "href": "posts/for_you.html#ok-time-to-say-goodbye",
    "title": "Finally, An Introduction: The Story Behind This Blog and Whatâ€™s Next",
    "section": "Ok, Time to Say Goodbye ğŸ‘‹",
    "text": "Ok, Time to Say Goodbye ğŸ‘‹\nThatâ€™s all for now, folks! See you soon, and take care! Love you all! ğŸ’–"
  },
  {
    "objectID": "posts/model_gpu.html",
    "href": "posts/model_gpu.html",
    "title": "How Much Memory Does Your Model Need on GPU? Letâ€™s Find Out!",
    "section": "",
    "text": "A Tool to Estimate Model Memory Usage on GPU\nHey there, fellow developers and curious minds! ğŸ–– Ever wondered how much juice you need to run a model on your GPU? Whether youâ€™re knee-deep in code or just dipping your toes into the tech waters, the question of resources has likely crossed your mind.\n\nBuilding a Tool to Estimate GPU Memory Usage\nGuess what? I built a tool for that! ğŸ‰ But hold up, before you get too excitedâ€¦ it only estimates the memory required to load the model onto a GPU. Yep, just thatâ€”no more, no less. If youâ€™re itching to test it out, hereâ€™s the link.\n\n\nWhy Only Estimate GPU Memory for Loading?\nYou might be asking, â€œWhy just the memory for loading?â€ Good question! Itâ€™s because to really figure out how much resources a model needs to run (whether training or inference), you actually need to run it. Yep, no shortcuts here.\nIt all depends on things like the size of your input/batch, the quality of your data (low-res images vs.Â high-res by eg), and a whole bunch of other factors. ğŸ¤¯ Oh, and donâ€™t forget the extra memory needed to store activations during the forward pass and gradients during the backward pass. The bigger the model, the more resources youâ€™ll need. Itâ€™s a classic case of go big or go home!\n\n\nSo, How Do I Know How Much Memory My Model Needs?\nHereâ€™s the deal: The memory a model uses for training is not the same as it uses for inference. Training requires more memory because it needs to store activations and gradients for backpropagation. Unfortunately, thereâ€™s no one-size-fits-all rule to tell you exactly how much memory youâ€™ll need, but we can make an educated guess.\nYou can start by using my tool to get a minimum memory estimate for loading the model. To estimate training memory, a rough approach is to multiply that number by 10, but keep in mind this is just a ballpark figure. The exact factor can vary depending on your model architecture and batch size. To be safe, always test with different configurations to find the sweet spot.\nDuring inference, models generally require less memory and run faster compared to training, but there are exceptions. Monitoring tools like nvidia-smi can help you keep track of actual GPU usage and avoid surprises. ğŸ“Š\n\n\nExample Time! ğŸ®\nLetâ€™s take a practical example. I want to fine-tune the Qwen 2 model (Qwen/Qwen2-0.5B-Instruct) on a GPU P100 with 16GB of memory on Kaggle. For this exemple, I was trying to fine-tune the model on a dataset of 10k samples with a batch size of 2 and a maximum of 800 tokens per input, on a NER task.\nWhen I load the model onto the GPU, it takes up around 2.6GB of memory. Not too bad, right? But when I try to train the model, you see in the image below, it maxes out all the resources. ğŸš€\n\n\n\nGPU Memory Usage\n\n\nSo, whatâ€™s the takeaway here? Even if your model loads fine, it might not run smoothly during training. Always keep an eye on your resources and adjust your model or batch size accordingly.\n\n\nV100 vs.Â A100 vs.Â Whatever GPU: What Difference Does It Make?\nSo, you might be wondering, â€œDoes it really matter which GPU I use?â€ Well, the short answer is: Absolutely! ğŸï¸\nItâ€™s like traveling from Canada to France. You could either fly or bike. Pretty obvious which one gets you there faster, right? The same logic applies to GPUsâ€”more powerful ones will get the job done quicker, especially when running large models.\nLetâ€™s break it down:\n\nPowerful GPUs like the A100 not only handle large models better but also have extra features, like mixed-precision training, that can reduce memory usage and speed things up. Theyâ€™re like the jet engines of the GPU worldâ€”fast, efficient, and capable of handling heavy workloads with ease.\nOlder or less powerful GPUs like the V100 can still do the job, but they might struggle with bigger models or more complex tasks. Itâ€™s like taking a slower flightâ€”still gets you there, but not as fast or smoothly.\nWhatever GPU you have, just remember: You need both memory and raw power to make your model run efficiently. If your GPU isnâ€™t powerful enough, your model might take longer to run or could even crash. And nobody wants that!\n\nSo, whether youâ€™re eyeing the latest A100 or making do with what youâ€™ve got, the takeaway is clear: A more powerful GPU means faster, more efficient model runs. If you need to go deeper, you can read this article too.\n\n\nAlternatives If Your Model Is Too Big\nGot a huge model with over a billion parameters? No worries, there are alternatives! You can try building a smaller model that uses fewer resources. Techniques like transfer learning, model quantization, or even using a smaller model altogether can help.\nHowever, keep in mind that these approaches might impact accuracy. Sometimes the trade-off isnâ€™t that significant, and you can even set up safeguards for uncertain predictions, allowing the user to choose the most appropriate response. ğŸ’¡\nFun fact: Iâ€™m working on a tool to anonymize text before feeding it to a large language model (LLM), and I use one of these techniques to run the model directly in the browser. Check it out here!\n\n\nTakeaways\nAlright, letâ€™s wrap things up:\n\nEstimating exact memory requirements for your model can be tricky, but my tool can give you a good starting point.\nThe more powerful your GPU, the faster and smoother your model will runâ€”so choose wisely!\nIf your model is too big, consider alternatives like downsizing or using techniques that reduce resource demands, though be mindful of potential accuracy trade-offs.\n\n\n\nGoodbye and Thanks!\nThanks for sticking with me through this post! I hope you found it helpful. If you have any thoughts, suggestions, or just want to say hi, feel free to reach out via email or drop an issue below. Iâ€™m always open to learning and would love to hear from you.\nTake care, and happy coding! ğŸš€\n\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "",
    "text": "AI Developer fluent in machine learning and web technologies, Iâ€™m passionate about leveraging AI to tackle real-world problems and create a tangible impact. My primary interests lie in revolutionizing industries such as food production and finance through innovative technological solutions. With a deep commitment to sharing knowledge and fostering collaboration, I actively contribute to open-source projects and engage with the tech community.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#experiences",
    "href": "about.html#experiences",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Experiences",
    "text": "Experiences\n\nLead Developer | Sept 2022 - Jul 2023\nAgroSfer | Cotonou, BÃ©nin\nLead development teams in the creation of a dynamic data aggregation system to feed analytical dashboards.\n\n\nFront-end Developer | Jun 2021 - Sept 2022\nAgroSfer | Cotonou, BÃ©nin\nStudied advanced software systems, focusing on practical and theoretical aspects of software engineering in Benin.\n\n\nInternship in Web Development | Mars 2021 - Mai 2021\nLa Vedette Media Digital | Cotonou, BÃ©nin\nDesign and development of the architecture for an intranet-based project management web platform for La Vedette Media.\n\n\nInternship in Web Development | June 2020 - Sept 2020\nNAUTILUS TECHNOLOGY | Cotonou, BÃ©nin\nInternship role focusing on web development using JavaScript, Angular, Laravel, and Git, solidifying foundational web development skills.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Skills",
    "text": "Skills\n\nArtificial Intelligence\nPython, Pytorch, Tensorflow, Keras\nMatplotlib, Seaborn, Numpy, Pandas, timm, fastai, torchvision, Poutyne\nSPSS Modeler, RStudio\n\n\nSoftware engineering\nPHP, Javascript, C, C++, Java\nAngular, ReactJs, VueJs, Cypress, Web API\nSQL, Mysql, Firebase, MongoDB\n\n\nOthers\nDevOps, CI/CD, UX Design, Scrum, Agile",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Education",
    "text": "Education\n\nMaÃ®trise en informatique - Intelligence Artificielle | Since Sept 2023\nUniversitÃ© Laval | QuÃ©bec, CA\nPursuing a Masterâ€™s degree focusing on Artificial Intelligence, enhancing skills in complex AI systems and machine learning implementations.\n\n\nSystÃ¨mes informatiques et Logiciels | Oct 2018 - Jul 2021\nLes COURS SONOU | Abomey-Calavi, BÃ©nin\nStudied advanced software systems, focusing on practical and theoretical aspects of software engineering in Benin.\n\n\nBaccalaurÃ©at Science Maths | AoÃ»t 2018\nCEG Application | Porto-Novo, BÃ©nin\nGraduated with a focus on Mathematics and Science, forming a strong analytical foundation.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#online-courses",
    "href": "about.html#online-courses",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Online courses",
    "text": "Online courses\n\nFrom Engineer to Technical Manager: A Survival Guide, Sundog Education\nUdemy - 2023",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "Konrad Tagnon Amen ALAHASSA",
    "section": "Interests",
    "text": "Interests\nAI, Software Engineering, Web Development, Reading, Traveling, Music, Movies, Animes, Swimming, Hiking, BasketBall, and Learning new things.",
    "crumbs": [
      "L'auteur"
    ]
  },
  {
    "objectID": "works.html",
    "href": "works.html",
    "title": "Personal works and other stuffs",
    "section": "",
    "text": "I want to share what Iâ€™ve accomplished as projects, in AI, in software engineering â€“ and maybe some other areas too. ğŸš€",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#in-ai",
    "href": "works.html#in-ai",
    "title": "Personal works and other stuffs",
    "section": "In AI",
    "text": "In AI\nItâ€™s been a while since I started my journey into AI. Here are some of the projects Iâ€™ve worked on:\n\nA web tool to anonymize text before feeding it to a large language model (LLM): Iâ€™m working on a tool that uses techniques like transfer learning and model quantization to run a large language model directly in the browser. Itâ€™s a fun project that I canâ€™t wait to complete and share with you all!\nA Tool to Estimate Model Memory Usage on GPU: I built a tool that estimates the memory required to load a model onto a GPU. Itâ€™s a handy tool for developers who want to how much resources they need to run a model on their GPU from Hugging Face for example.\n\nand itâ€™s just the beginning! ğŸš€",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#in-software-engineering",
    "href": "works.html#in-software-engineering",
    "title": "Personal works and other stuffs",
    "section": "In software engineering",
    "text": "In software engineering\nBefore starting my masterâ€™s degree in Artificial Intelligence, I worked on a lot of projects in software engineering. Here are some of them that I found interesting:\n\nA blog using Vue.js: I built a blog using Vue.js, SemanticUi, NodeJs, ExpressJs and MongoDB. It was a fun project that helped me learn more about all these technologies. I built a frontend and a backend for this project.\nI tried to clone Trello ğŸ˜…: I tried to build a Trello clone using React and Laravel. It was a fun project that I didnâ€™t finish but I learned a lot about React and Laravel and how to setup live messaging between frontend and backend using Laravel Echo.\nLarangular: a Laravel and Angular project: I built an invoice app using Laravel and Angular. The idea is to have a sample app where for an order, you add a list of the articles, and you generate an invoice. You can also export the invoice as a PDF. The backend is built using Laravel and all of them is under same repository. Bad practice, I know, but it was a fun project :D\nmytodoapp: a todo app using React: I built a sample todo app using React. It was my first project on my web development journey. I learned a lot about React, Git and how to deploy a React app on GitHub pages.\n\nand a lot more you can find on my GitHub. ğŸŒŸ",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "works.html#found-this-interesting",
    "href": "works.html#found-this-interesting",
    "title": "Personal works and other stuffs",
    "section": "Found this interesting?",
    "text": "Found this interesting?\nLet me know if you have any comments or ideas on how I can improve. Iâ€™m all ears! ğŸ‘‚ Iâ€™ll be updating regularly on my progress and any news, so keep an eye out for more. Thank you for reading! ğŸ’–",
    "crumbs": [
      "My works"
    ]
  },
  {
    "objectID": "posts/lessons_from_calf_face.html",
    "href": "posts/lessons_from_calf_face.html",
    "title": "What I Learned from Calf Face ğŸ˜Š",
    "section": "",
    "text": "Alright, letâ€™s be realâ€”when we started, we had a tiny data problem. Weâ€™re talking less than 200 images of calf faces (yep, calf faces!). And trust me, thatâ€™s nowhere near enough if we want to build a robust detection model. ğŸ®\nBut no worries, we had a plan! First, we took YOLOv8 and fine-tuned it with some clever data augmentation. We went from a measly 178 images to a whopping 890â€”thanks to a combo of GaussianBlur, MedianBlur, Sharpen, Flip, and good olâ€™ rotation (between 10 to 20 degrees, nothing too wild).\nNow, the goal was to keep the transformations realistic. No crazy color changes or outlandish rotations that might produce data that doesnâ€™t even exist in the real worldâ€”because thatâ€™s how you end up with a model that thinks cows fly. ğŸ„âœˆï¸\nAnd guess what? After this magic data expansion, we saw a small bump in mAP50 (mean Average Precision) and other YOLO metrics. We went from 0.803 with basic data augmentation of YoloV8 to 0.891 on the mAP score after 10 epochs. ğŸ“ˆ\nHereâ€™s a quick table showing the before and after, because data geeks love tables:\n\n\n\nMetric\nBefore\nAfter\n\n\n\n\nmAP50\n0.803\n0.891\n\n\nmAP50-90\n0.503\n0.502\n\n\nPrecision\n0.992\n1\n\n\nRecall\n0.778\n0.769\n\n\n\nNow, before anyone screams â€œOverfitting!â€â€”hold up. Yes, the boost in performance might make it seem like thatâ€™s whatâ€™s happening, but weâ€™re confident thatâ€™s not the case. Why? Because the video data we plan to use for detection closely resembles the training data. In other words, the modelâ€™s doing exactly what we need it to do: detect calves in environments that are super similar to the ones itâ€™s been trained on. ğŸ¯\nOh, and by the wayâ€”the metrics you see in that table? Those arenâ€™t based on the augmented data. Nope! Theyâ€™re from a subset of the original 175 images, which we held back specifically as our test set. So, the numbers here reflect real, unaltered images, giving us a more honest assessment of the modelâ€™s performance. ğŸ’ª"
  },
  {
    "objectID": "posts/lessons_from_calf_face.html#small-data-big-dreams",
    "href": "posts/lessons_from_calf_face.html#small-data-big-dreams",
    "title": "What I Learned from Calf Face ğŸ˜Š",
    "section": "",
    "text": "Alright, letâ€™s be realâ€”when we started, we had a tiny data problem. Weâ€™re talking less than 200 images of calf faces (yep, calf faces!). And trust me, thatâ€™s nowhere near enough if we want to build a robust detection model. ğŸ®\nBut no worries, we had a plan! First, we took YOLOv8 and fine-tuned it with some clever data augmentation. We went from a measly 178 images to a whopping 890â€”thanks to a combo of GaussianBlur, MedianBlur, Sharpen, Flip, and good olâ€™ rotation (between 10 to 20 degrees, nothing too wild).\nNow, the goal was to keep the transformations realistic. No crazy color changes or outlandish rotations that might produce data that doesnâ€™t even exist in the real worldâ€”because thatâ€™s how you end up with a model that thinks cows fly. ğŸ„âœˆï¸\nAnd guess what? After this magic data expansion, we saw a small bump in mAP50 (mean Average Precision) and other YOLO metrics. We went from 0.803 with basic data augmentation of YoloV8 to 0.891 on the mAP score after 10 epochs. ğŸ“ˆ\nHereâ€™s a quick table showing the before and after, because data geeks love tables:\n\n\n\nMetric\nBefore\nAfter\n\n\n\n\nmAP50\n0.803\n0.891\n\n\nmAP50-90\n0.503\n0.502\n\n\nPrecision\n0.992\n1\n\n\nRecall\n0.778\n0.769\n\n\n\nNow, before anyone screams â€œOverfitting!â€â€”hold up. Yes, the boost in performance might make it seem like thatâ€™s whatâ€™s happening, but weâ€™re confident thatâ€™s not the case. Why? Because the video data we plan to use for detection closely resembles the training data. In other words, the modelâ€™s doing exactly what we need it to do: detect calves in environments that are super similar to the ones itâ€™s been trained on. ğŸ¯\nOh, and by the wayâ€”the metrics you see in that table? Those arenâ€™t based on the augmented data. Nope! Theyâ€™re from a subset of the original 175 images, which we held back specifically as our test set. So, the numbers here reflect real, unaltered images, giving us a more honest assessment of the modelâ€™s performance. ğŸ’ª"
  },
  {
    "objectID": "posts/lessons_from_calf_face.html#choosing-the-right-model-training-where-things-get-real",
    "href": "posts/lessons_from_calf_face.html#choosing-the-right-model-training-where-things-get-real",
    "title": "What I Learned from Calf Face ğŸ˜Š",
    "section": "Choosing the Right Model & Training: Where Things Get Real ğŸ¯",
    "text": "Choosing the Right Model & Training: Where Things Get Real ğŸ¯\nOkay, so once we had the data sorted, it was time to pick the right modelâ€”and honestly, thatâ€™s where things got a little complicated. Not only did I need something that would crush the performance side of things, but I also needed to be able to explain why the model made the decisions it did. You know, in case anyone asks the big question: â€œWhy did your model do that?â€ ğŸ˜…\n\nThe Model Line-Up ğŸ†\nFor images, I went through a few options before landing on the right one. I tried DeepLabV3, EfficientNet, InceptionV3, and even Unet (which I ended up abandoningâ€”long story ğŸ« ). I also gave ViT and a LSTM+CNN combo a shot, but, wellâ€¦ letâ€™s just say I didnâ€™t get around to finishing that one. Too much on my plate!\nFor videos, it was another story. I experimented with TimeSformer, ViViT, and VideoMAE to handle the moving pictures. ğŸ¥\n\n\nCracking the Modelâ€™s Decisions: Interpretability with OmnixAI ğŸ§ \nWhen it came to understanding why the models made specific predictions, I leaned on some awesome interpretability algorithms from OmnixAI. These tools helped me peek under the hood of the models and get a better sense of their thought process.\nI used a mix of techniques like GradCAM, LIME, Score-CAM, and even SmoothGrad, GuidedBP, and LayerCAM to visualize what parts of the image the model was focusing on. Each one gave me a slightly different view of how the model was processing the data, which made interpreting results a whole lot easier (and way more fun to explain ğŸ˜).\nAnd I also even trained INTR, a transformer-based model specifically designed for interpretability. âœ¨\n\n\nBuilding a Killer Dataset Split ğŸ§©\nNext up, I had to make sure my dataset was split in a way that would allow for a solid comparison between models. Hereâ€™s how I did it:\nUsing the YOLO model I mentioned earlier, I went through almost 1 hour of video showing calves approaching a feeder. I grabbed the 10 seconds before each calf started feeding and extracted images where their cute little faces were visible. In the end, I had 1,349 videos and 7,687 images, representing 76 unique calves.\n\n\n\nNumber of videos per calf status\n\n\nTo keep things clean, I handpicked 68 videos where I verified that each calf was actually approaching the feeder (with no noise), and used those videos as my test set. The images from those videos? They became my image test set (283 images in total).\n\n\nTwo Different Training Sets? Why Not? ğŸ¤·â€â™‚ï¸\nI built two separate training sets just for fun (and science, of course):\n\nFull Set: This one included all the remaining images and videos, no holds barred.\nLimited Set: Hereâ€™s where things got interesting. I only used one video and two images per calf, per health statusâ€”â€œhealthy,â€ â€œdiarrhea,â€ or â€œpneumonia.â€ My logic? Since some images were super similar (I sampled about 30 images evenly from each 10-second video), I figured the model could easily overfit. I wanted to see how it would perform with less redundant data.\n\nYeah, itâ€™s a pretty naive approach, but it sped up my work and saved some headaches. Plus, even though about 40% of the data was, well, garbage ğŸ—‘ï¸, the rest of it more than made up for it. ğŸ’ª\nI should mentioned that those calf in the test set were not in the training set, so the model never saw them before. ğŸ„\n\n\nPro Tip: Use MLflow! ğŸ› ï¸\nOh, one last thing. If youâ€™re planning on doing so much training like this, seriously, do yourself a favor and use MLflow to track your experiments. I didnâ€™t use it for this project (donâ€™t ask, it was a mess, I swear! ğŸ¤¦â€â™‚ï¸), but if I had to do it all over again, MLflow wouldâ€™ve saved me SO much time and effort. Lesson learned!"
  },
  {
    "objectID": "posts/lessons_from_calf_face.html#results-the-good-the-bad-and-the-calves",
    "href": "posts/lessons_from_calf_face.html#results-the-good-the-bad-and-the-calves",
    "title": "What I Learned from Calf Face ğŸ˜Š",
    "section": "Results: The Good, the Bad, and the Calves ğŸ„",
    "text": "Results: The Good, the Bad, and the Calves ğŸ„\nYou can check out all the detailed results and model performances in the dedicated repo for this project (because sharing is caring, right? ğŸ“‚). But while I didnâ€™t get a chance to completely wrap up the work (classic researcher life ğŸ˜…), there are a few important highlights that are worth mentioning.\n\nThe Curious Case of the INTR Model ğŸ¤”\n\n\n\nHow the INTR model works. From the original paper\n\n\nSo, letâ€™s talk about the INTR model. The original paper boasted that this model could better explain its decisions, which sounded perfect for what I needed. But after fine-tuning it on my dataset? Yeah, not quite the same result. ğŸ¤·â€â™‚ï¸\n\n\n\nHow the INTR performs on my dataset\n\n\nWhy? Well, hereâ€™s where the data shift kicked in. The pre-trained model had been trained on images where the object of interest was neatly placed in the center of the frame (picture-perfect). My dataset? Not so much. My calves were sometimes, all over the place, doing their own thing, and not staying center-stage. ğŸ˜¬\n\n\nThe â€œSame Old Backgroundâ€ Problem ğŸŒ¾\n\n\n\nFun image of mean calf face. I used all images extracted from the videos to compute it.\n\n\nAnother issue I ran into: background consistency. Since all my training data came from the same farm, the background was pretty much always the same. So, while the model learned to perform decently well on that data, it didnâ€™t generalize well when I tested it with images from a different farm. Different farms = different environments. And as we all know, no two farms are exactly alike, right? ğŸ¡ ğŸ„\nItâ€™s not the modelâ€™s faultâ€”itâ€™s just how things are. If the goal is to have a model that works across various farms, we either need a ton more data from different environments, or maybe we should consider building farm-specific models. Unless, of course, we want to standardize every farm, which, letâ€™s face it, isnâ€™t going to happen. ğŸ¤·â€â™€ï¸\n\n\nIs the Model Actually Learning the Right Things? ğŸ¤¨\nHereâ€™s the tricky part: The model performed relatively well. Butâ€”big butâ€”it could be learning patterns we donâ€™t want it to focus on. Maybe itâ€™s using the calfâ€™s face color, or maybe the background is playing a bigger role in its decision-making than weâ€™d like to admit. In short, even though the performance metrics look good on paper, Iâ€™d recommend handling them with care. ğŸ“Š\n\n\nWords of Wisdom from My Professor ğŸ‘¨â€ğŸ«\nMy supervisor always says we shouldnâ€™t expect miracles from these models. After all, itâ€™s hard for even a human to just look at a calf and determine its health status purely based on appearance. Add in the fact that weâ€™re working with limited data, and yeahâ€”it was always going to be a challenge.\nBut the whole point of this project was to test, explore, and see how a deep learning approach would hold up, despite those challenges. So, while the results may not be earth-shattering, the insights weâ€™ve gained are super valuable for refining future models. ğŸ’¡"
  },
  {
    "objectID": "posts/lessons_from_calf_face.html#have-we-answered-the-big-question-can-we-detect-calf-health-status-from-images-or-videos",
    "href": "posts/lessons_from_calf_face.html#have-we-answered-the-big-question-can-we-detect-calf-health-status-from-images-or-videos",
    "title": "What I Learned from Calf Face ğŸ˜Š",
    "section": "Have We Answered the Big Question? Can We Detect Calf Health Status from Images or Videos? ğŸ¤”",
    "text": "Have We Answered the Big Question? Can We Detect Calf Health Status from Images or Videos? ğŸ¤”\nShort answer? Not yet. ğŸ›‘\nTo be honest, I think we need a better dataset to get the results weâ€™re aiming for. Thereâ€™s definitely potentialâ€”maybe we can identify new features or patterns to help detect diseases more accurately, or perhaps the model can be trained to pick up on the same cues humans use when assessing calf health. But realistically, thatâ€™s going to take a lot more work than what Iâ€™ve done so far. ğŸ„ğŸ’»\nOne major thing Iâ€™ve learned is that using pre-trained models (whether theyâ€™re foundation models or not) on real-life problems takes way more effort, data, and attention to detail than I ever expected. When you watch those flashy demos where models seem to perform flawlessly, itâ€™s easy to think, â€œI got this.â€ But in reality? The process is a bit messier, and it involves a lot more tweaking than the demo might let on. ğŸ˜…\nMaybe I didnâ€™t make all the right decisionsâ€”choosing the best model, or even formulating the right hypothesesâ€”but hey, thatâ€™s how we learn, right? If you have thoughts, feedback, or think thereâ€™s something I missed, feel free to comment and let me know. Letâ€™s keep the conversation going and figure this out together! ğŸ¤"
  },
  {
    "objectID": "posts/lessons_from_calf_face.html#and-thats-a-wrap",
    "href": "posts/lessons_from_calf_face.html#and-thats-a-wrap",
    "title": "What I Learned from Calf Face ğŸ˜Š",
    "section": "And Thatâ€™s a Wrap! ğŸ¬",
    "text": "And Thatâ€™s a Wrap! ğŸ¬\nUntil next time, keep learning, keep growing, and keep exploring the world of AI. Itâ€™s a wild ride, but hey, someoneâ€™s gotta do it! ğŸš€"
  },
  {
    "objectID": "posts/dataset-converter.html",
    "href": "posts/dataset-converter.html",
    "title": "Passer de Yolo Ã  CocoDetection, câ€™est possible !",
    "section": "",
    "text": "De quoi sâ€™agit-il ?\nDans ce post, je montre comment convertir un dataset YOLO en format COCO en utilisant la bibliothÃ¨que Python globox, une solution pratique que jâ€™ai rÃ©cemment appliquÃ©e pour un projet de segmentation dâ€™instance. globox permet Ã©galement de faire lâ€™inverse, ainsi que de nombreuses autres conversions de formats de datasets. Je vous invite Ã  explorer ce package pour dÃ©couvrir toutes les possibilitÃ©s quâ€™il offre.\n\n\nOk, Ã§a donne quoi ?\n\n\nCode\n!pip install globox\n!pip install pycocotools\n\n\n\n\nCode\nfrom pathlib import Path\nfrom PIL import Image\nfrom torchvision.datasets import CocoDetection\nfrom globox import AnnotationSet\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\n\n\nLe code ci-dessous initialise trois chemins en utilisant la bibliothÃ¨que Path de Python. Les chemins label_path et image_path pointent respectivement vers les dossiers contenant les Ã©tiquettes et les images du dataset YOLO. Ensuite, le chemin save_file est dÃ©fini pour sauvegarder le fichier de sortie au format COCO. Enfin, la commande annotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path) crÃ©e un ensemble dâ€™annotations Ã  partir des Ã©tiquettes YOLO et des images associÃ©es, prÃªt Ã  Ãªtre converti au format COCO.\n\nlabel_path = Path(\"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/labels\")\nimage_path = Path(\"/kaggle/input/medleaves-medicinal-plant-leaves-dataset/MedLeaves/MedLeaves/train/images\") \n\nsave_file = Path(\"/kaggle/working/medleaves-coco.json\")\n\nannotations = AnnotationSet.from_yolo_v7(label_path, image_folder=image_path)\n\nLa commande annotations.show_stats() est utilisÃ©e pour afficher des statistiques sur lâ€™ensemble dâ€™annotations crÃ©Ã© prÃ©cÃ©demment. Cette mÃ©thode fournit un rÃ©sumÃ© utile des donnÃ©es annotÃ©es, comme le nombre total dâ€™images, dâ€™annotations, et la distribution des catÃ©gories dâ€™objets. Cela permet de vÃ©rifier rapidement lâ€™Ã©tat et la qualitÃ© des annotations avant de procÃ©der Ã  des opÃ©rations de conversion ou dâ€™entraÃ®nement de modÃ¨les.\n\nannotations.show_stats()\n\n      Database Stats      \nâ”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\nâ”ƒ Label â”ƒ Images â”ƒ Boxes â”ƒ\nâ”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\nâ”‚ 0     â”‚    415 â”‚  2322 â”‚\nâ”‚ 1     â”‚    408 â”‚   498 â”‚\nâ”‚ 10    â”‚    416 â”‚   588 â”‚\nâ”‚ 11    â”‚    412 â”‚   612 â”‚\nâ”‚ 12    â”‚    414 â”‚  2401 â”‚\nâ”‚ 13    â”‚    410 â”‚   810 â”‚\nâ”‚ 14    â”‚    412 â”‚  2750 â”‚\nâ”‚ 15    â”‚    408 â”‚  1605 â”‚\nâ”‚ 16    â”‚    411 â”‚  2237 â”‚\nâ”‚ 17    â”‚    411 â”‚  2662 â”‚\nâ”‚ 18    â”‚    409 â”‚   682 â”‚\nâ”‚ 19    â”‚    413 â”‚  5032 â”‚\nâ”‚ 2     â”‚    414 â”‚  5830 â”‚\nâ”‚ 20    â”‚    405 â”‚   882 â”‚\nâ”‚ 21    â”‚    415 â”‚  2892 â”‚\nâ”‚ 22    â”‚    409 â”‚   540 â”‚\nâ”‚ 23    â”‚    414 â”‚  1275 â”‚\nâ”‚ 24    â”‚    411 â”‚   727 â”‚\nâ”‚ 25    â”‚    407 â”‚  4778 â”‚\nâ”‚ 26    â”‚    413 â”‚  1255 â”‚\nâ”‚ 27    â”‚    406 â”‚   997 â”‚\nâ”‚ 28    â”‚    411 â”‚   618 â”‚\nâ”‚ 29    â”‚    409 â”‚  1143 â”‚\nâ”‚ 3     â”‚    411 â”‚  1584 â”‚\nâ”‚ 4     â”‚    405 â”‚  3082 â”‚\nâ”‚ 5     â”‚    405 â”‚  1275 â”‚\nâ”‚ 6     â”‚    416 â”‚   769 â”‚\nâ”‚ 7     â”‚    403 â”‚   570 â”‚\nâ”‚ 8     â”‚    409 â”‚  1563 â”‚\nâ”‚ 9     â”‚    410 â”‚  2475 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Total â”‚  12312 â”‚ 54454 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nCâ€™est ici que la magie Ã  lieu ! La commande annotations.save_coco(save_file, auto_ids=True) sauvegarde lâ€™ensemble dâ€™annotations dans un fichier au format COCO, en utilisant le chemin spÃ©cifiÃ© par save_file. Lâ€™option auto_ids=True indique que les identifiants pour les annotations et les images seront gÃ©nÃ©rÃ©s automatiquement si nÃ©cessaire.\n\nannotations.save_coco(save_file, auto_ids=True)\n\nVisualisons ensuite le rÃ©sultat, mais en chargeant les donnÃ©es au format CocoDetection. Le code ci-dessous fait cela en utilisant la classe CocoDetection de la bibliothÃ¨que PyTorch.\nLe paramÃ¨tre root est dÃ©fini avec le chemin vers le dossier des images (image_path), tandis que annFile spÃ©cifie le chemin du fichier dâ€™annotations au format COCO (save_file).\nEnfin, train_data[0][0] accÃ¨de Ã  la premiÃ¨re image du dataset, afin ainsi de le visualiser.\n\ntrain_data = CocoDetection(root = image_path, annFile = save_file, transform = None)\ntrain_data[0][0]\n\n\n\n\n\n\n\n\nProfitons pour visualiser une image du dataset avec ses annotations (bbox) superposÃ©es sur lâ€™image.\n\n\nCode\n# Obtenir la premiÃ¨re image et ses annotations\nimage, annotations = train_data[0]\n\n# Convertir l'image PIL en tableau numpy pour l'affichage\nimage_np = np.array(image)\n\n# CrÃ©er un graphique\nfig, ax = plt.subplots(1)\nax.imshow(image_np)\n\n# Tracer chaque boÃ®te englobante\nfor ann in annotations:\n    bbox = ann['bbox']\n    # Le format de boÃ®te englobante COCO est [x, y, largeur, hauteur]\n    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n\n# Afficher le graphique\nplt.show()\n\n\n\n\n\n\n\n\n\nJâ€™espÃ¨re que cela vous sera utile. Nâ€™hÃ©sitez pas Ã  me contacter si vous avez des questions ou des recommendations. Ã€ la prochaine ! :-)\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/anonymizer_intro.html",
    "href": "posts/anonymizer_intro.html",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "",
    "text": "Hey folks! ğŸ™Œ\nEver find yourself using those cool new LLMs like ChatGPT for pretty much everythingâ€”whether itâ€™s writing emails, drafting cover letters, or even creating blog content (I know I do! ğŸ˜…)? Itâ€™s like having your personal assistant right there in your browser, making life so much easier. But, waitâ€¦what happens when your prompts contain personal details? ğŸ˜¬\nYou know, that awkward moment when you realize youâ€™ve shared too much in a prompt? Yeah, been there, done that. So, I figured, why not build a tool that handles this issue and keeps things private? ğŸ’¡"
  },
  {
    "objectID": "posts/anonymizer_intro.html#how-i-found-myself-working-on-this",
    "href": "posts/anonymizer_intro.html#how-i-found-myself-working-on-this",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "How I Found Myself Working on This ğŸ› ï¸",
    "text": "How I Found Myself Working on This ğŸ› ï¸\nRecently, I noticed just how much Iâ€™ve been relying on LLMs for personal stuffâ€”writing emails, tweaking my CV, crafting blogs, even coding! But every time I use them, I have to manually remove or anonymize personal info in my prompts. Talk about tedious! ğŸ˜©\nIt got to a point where I was doing this so often that I thought, â€œThereâ€™s got to be a better way!â€ And so, the idea of building a tool that runs entirely offline, right in your browser, was born. ğŸš€"
  },
  {
    "objectID": "posts/anonymizer_intro.html#the-idea",
    "href": "posts/anonymizer_intro.html#the-idea",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "The Idea ğŸ’¡",
    "text": "The Idea ğŸ’¡\nItâ€™s super simple! Just paste your text, and voilaâ€”it anonymizes it. Now, you can safely use it in your prompts without worrying about personal details leaking out. âœ¨\n\nExample Time! âœï¸\nImagine youâ€™ve have to send an email to a customer about an upcoming event at your coffee shop. Hereâ€™s what it might look like:\n\nSubject: Join us for the Grand Renovation of New York - Exciting Changes Await!\nDear John Green,\nWe hope this email finds you well and enjoying your favorite vanilla latte large and banana muffin at our coffee shop! As a valued and loyal customer, we are thrilled to share some exciting news with you.\nMark your calendar for January 4, 2023, as we invite you to join us for the grand renovation of our coffee shop at New York! We are embarking on a journey to create an even better and cozier space for you to relax, indulge, and enjoy your favorite beverages and treats.\nYou have been a cherished part of our coffee shop family for an incredible 5 years, and we cannot wait to show our appreciation by unveiling a fresh, modern look that will elevate your coffee shop experience. The renovation will bring about stylish and comfortable seating areas, captivating artwork, and state-of-the-art equipment to enhance the quality of our offerings.\nThe grand reopening event promises to be a celebration filled with delightful surprises. As our special guest, you will be among the first to explore the redesigned space and sample our latest menu additions. Immerse yourself in the inviting ambiance, meet our talented baristas, and indulge in the aroma of freshly brewed coffee that is synonymous with our coffee shop. We value your opinion, John Green, and would be thrilled to hear your thoughts and feedback on our newly renovated space. As always, our commitment remains to provide you with exceptional customer service and the highest quality products that you know and love.\nSave the date: January 4, 2023! We cannot wait to welcome you to the grand renovation of our coffee shop at New York. Together, letâ€™s embark on this exciting journey and create countless new memories over aromatic coffees and delectable treats.\nThank you for being an integral part of our coffee shopâ€™s success. We look forward to continuing to serve you and make your coffee experiences unforgettable.\nWarmest regards,\n\nYou paste it into the tool, and poofâ€”it replaces those details with anonymous IDs. ğŸ•µï¸â€â™€ï¸\n\nSubject: Join us for the Grand Renovation of aLOCATION_524a - Exciting Changes Await!\nDear aPEOPLE_872a,\nWe hope this email finds you well and enjoying your favorite vanilla latte large and banana muffin at our coffee shop! As a valued and loyal customer, we are thrilled to share some exciting news with you.\nMark your calendar for aDATE_988a, as we invite you to join us for the grand renovation of our coffee shop at aLOCATION_524a! We are embarking on a journey to create an even better and cozier space for you to relax, indulge, and enjoy your favorite beverages and treats.\nYou have been a cherished part of our coffee shop family for an incredible 5 years, and we cannot wait to show our appreciation by unveiling a fresh, modern look that will elevate your coffee shop experience. The renovation will bring about stylish and comfortable seating areas, captivating artwork, and state-of-the-art equipment to enhance the quality of our offerings.\nThe grand reopening event promises to be a celebration filled with delightful surprises. As our special guest, you will be among the first to explore the redesigned space and sample our latest menu additions. Immerse yourself in the inviting ambiance, meet our talented baristas, and indulge in the aroma of freshly brewed coffee that is synonymous with our coffee shop. We value your opinion, aPEOPLE_872a, and would be thrilled to hear your thoughts and feedback on our newly renovated space. As always, our commitment remains to provide you with exceptional customer service and the highest quality products that you know and love.\nSave the date: aDATE_988a! We cannot wait to welcome you to the grand renovation of our coffee shop at aLOCATION_524a. Together, letâ€™s embark on this exciting journey and create countless new memories over aromatic coffees and delectable treats.\nThank you for being an integral part of our coffee shopâ€™s success. We look forward to continuing to serve you and make your coffee experiences unforgettable.\nWarmest regards,\n\nThen, you can use that anonymized text in your LLM prompt. Once you get the LLMâ€™s response, you paste it back into the tool, and it replaces the IDs with your original info. As simple as that! ğŸ˜"
  },
  {
    "objectID": "posts/anonymizer_intro.html#how-to-try-it",
    "href": "posts/anonymizer_intro.html#how-to-try-it",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "How to Try It? ğŸ¤”",
    "text": "How to Try It? ğŸ¤”\nExcited to give it a spin? You can check it out here. Itâ€™s still a work in progress (so donâ€™t judge me too harshly! ğŸ˜œ), but the full offline version is coming soon.\nFor now, you can test it out and share your thoughts with me. Your feedback means the world! ğŸŒ"
  },
  {
    "objectID": "posts/anonymizer_intro.html#next-steps",
    "href": "posts/anonymizer_intro.html#next-steps",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "Next Steps ğŸš€",
    "text": "Next Steps ğŸš€\nHereâ€™s what Iâ€™ve got cooking for the future:\n\nTurning it into a browser extension that automatically catches text you paste into input boxes, helping protect your identity on the fly.\nExtending the tool to handle PII (Personally Identifiable Information) with customizable options for what to anonymize.\nAnd so much more! ğŸ‰"
  },
  {
    "objectID": "posts/anonymizer_intro.html#but-why",
    "href": "posts/anonymizer_intro.html#but-why",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "But Why? ğŸ¤·â€â™‚ï¸",
    "text": "But Why? ğŸ¤·â€â™‚ï¸\nGood question! There are already some awesome tools out there, like this one and this GitHub repo. But none of them are quite what I was looking forâ€”so I decided to share mine! ğŸ˜„\nIf youâ€™ve got any comments, feel free to reach out or open an issue. Iâ€™m all ears! ğŸ‘‚"
  },
  {
    "objectID": "posts/anonymizer_intro.html#it-was-fun-and-educational",
    "href": "posts/anonymizer_intro.html#it-was-fun-and-educational",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "It Was Fun and Educational! ğŸ“",
    "text": "It Was Fun and Educational! ğŸ“\nBuilding this tool was such a learning experience. I dove into ONNX, WebGPU, WebAssembly, Transformer.js, Hugging Face Inference, Angular 18, and so much more. And, boy, did I learn a lot! ğŸ’»\n\nAlso, a Few Things I Noticedâ€¦ ğŸ§\n\nRunning an LLM in the browser without a GPU can be super slow. ğŸ¢\nNot all browsers support WebGPU, so thatâ€™s a challenge.\nTo run a model in the browser, you need a really light model, preferably quantized and optimized for the web (like with Olive, for example). Ideally, it should run on WebGPU for the best performance."
  },
  {
    "objectID": "posts/anonymizer_intro.html#final-thoughts",
    "href": "posts/anonymizer_intro.html#final-thoughts",
    "title": "How I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)",
    "section": "Final Thoughtsâ€¦ ğŸ‰",
    "text": "Final Thoughtsâ€¦ ğŸ‰\nOverall, itâ€™s been a fantastic experience. Version 2 is already in the works, so stay tuned! ğŸ˜\nThanks so much for reading, and donâ€™t hesitate to reach out with your thoughts.\nCheers! ğŸ¥‚"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "multiverse",
    "section": "",
    "text": "What I Learned from Calf Face ğŸ˜Š\n\n\n\n\n\n\nWork Projects\n\n\nTips & Tricks\n\n\n\nIn this post, we dive into the challenges of using deep learning models to detect calf health status from images and videos. Spoiler: Weâ€™re not quite there yet, but the journey is full of insights and potential for future improvements!\n\n\n\n\n\n9 min.\n\n\n\n\n\n\n\nHow Much Memory Does Your Model Need on GPU? Letâ€™s Find Out!\n\n\n\n\n\n\nTips & Tricks\n\n\nPersonal Projects\n\n\n\nAn exploration of GPU memory requirements for running models and how to estimate them with a handy tool.\n\n\n\n\n\n5 min.\n\n\n\n\n\n\n\nHow I Built a Tool to Anonymize Text for LLMs (and Why You Might Need It Too!)\n\n\n\n\n\n\nTips & Tricks\n\n\nPersonal Projects\n\n\n\nDiscover a handy tool that anonymizes your text before using it in LLMs, keeping your personal information safe and sound!\n\n\n\n\n\n7 min.\n\n\n\n\n\n\n\nFinally, An Introduction: The Story Behind This Blog and Whatâ€™s Next\n\n\n\n\n\n\nPersonal Projects\n\n\nUpdates\n\n\n\nIn this post, I finally take the time to properly introduce my blog, explain its purpose, and share why Iâ€™ve been away for the past few months. Itâ€™s a fresh start and an exciting journey ahead!\n\n\n\n\n\n3 min.\n\n\n\n\n\n\n\nPasser de Yolo Ã  CocoDetection, câ€™est possible !\n\n\n\n\n\n\nTips & Tricks\n\n\nTutorial\n\n\n\nJe partage dans cet article une mÃ©thode simple pour convertir un dataset YOLO au format COCO, ou Ã  un autre, une solution que jâ€™ai trouvÃ©e essentielle lors dâ€™un rÃ©cent projet de segmentation dâ€™instance.\n\n\n\n\n\n2 min.\n\n\n\n\n\n\nAucun article correspondant\n\n Retour au sommet",
    "crumbs": [
      "Blog"
    ]
  }
]